{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Language Modelling\n",
    "**Étudiante: VU Thi Hai Yen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A (very small) introduction to pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Tensors are very similar to Numpy arrays, with the added benefit of being usable on GPU. For a short tutorial on various methods to create tensors of particular types, see [this link](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py).\n",
    "The important things to note are that Tensors can be created empty, from lists, and it is very easy to convert a numpy array into a pytorch tensor, and inversely.\n",
    "One very important way to manipulate tensors that is different from numpy is the method ```.view``` which is used, as ```reshape```, to change the shape of a tensor. The difference is that ```.view``` will avoid making a copy of the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.LongTensor(5)\n",
    "b = torch.LongTensor([5])\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2])\n",
    "b = torch.FloatTensor([3])\n",
    "\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interest in us using Pytorch is the ```autograd``` package. ```torch.Tensor```objects have an attribute ```.requires_grad```; if set as True, it starts to track all operations on it. When you finish your computation, can call ```.backward()``` and all the gradients are computed automatically (and stored in the ```.grad``` attribute).\n",
    "\n",
    "One way to easily cut a tensor from the computational once it is not needed anymore is to use ```.detach()```.\n",
    "More info on automatic differentiation in pytorch on [this link](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1456,  0.0138, -0.0051],\n",
      "        [-0.4318,  0.1507, -0.3429]], requires_grad=True)\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([0.1182, 0.3206], requires_grad=True)\n",
      "Initial loss:  1.408055067062378\n",
      "dL/dw:  tensor([[ 0.4195,  0.0227,  0.6357],\n",
      "        [-0.1008,  0.6632,  0.3723]])\n",
      "dL/db:  tensor([-0.2766,  0.4994])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "for name, p in linear.named_parameters():\n",
    "    print(name)\n",
    "    print(p)\n",
    "\n",
    "# Build loss function - Mean Square Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('Initial loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after one update:  1.3932287693023682\n"
     ]
    }
   ],
   "source": [
    "# You can perform gradient descent manually, with an in-place update ...\n",
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after one update: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after two updates:  1.3788959980010986\n"
     ]
    }
   ],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of the model.\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# By default, gradients are accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "# is called. Before the backward pass, we need to use the optimizer object to zero all of the\n",
    "# gradients.\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Calling the step function on an Optimizer makes an update to its parameters\n",
    "optimizer.step()\n",
    "\n",
    "# Print out the loss after the second step of gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after two updates: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a ```Dictionary``` class, that we are going to use to create a vocabulary for our text data. The goal here is to have a convenient tool, with easy access to any information we could need:\n",
    "- A python dictionary ```word2idx``` allowing easy transformation of tokenized text into indexes\n",
    "- A list ```idx2word```, allowing us to find the word corresponding to an index (for interpretation and generation)\n",
    "- A python dictionary ```counter``` used to build the vocabulary, that can provide us with frequency information if needed. \n",
    "- The ```total``` count of words in the dictionary.\n",
    "\n",
    "Important: The data that we are going to use are already pre-processed so we don't need to create special tokens and control the size of the vocabulary ourselves. However, when the text data is raw, methods to preprocess it conveniently should be added here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.counter = {}\n",
    "        self.total = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "            self.counter.setdefault(word, 0)\n",
    "        self.counter[word] += 1\n",
    "        self.total += 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      " \n",
      "\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./wikitext-2/train.txt', 'r', encoding=\"utf8\") as f:\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"': 60,\n",
      " '(': 9,\n",
      " ')': 18,\n",
      " ',': 12,\n",
      " '.': 14,\n",
      " '2011': 44,\n",
      " '3': 6,\n",
      " ':': 7,\n",
      " '<unk>': 8,\n",
      " '=': 0,\n",
      " '@-@': 29,\n",
      " 'Battlefield': 17,\n",
      " 'Chronicles': 2,\n",
      " 'Europan': 70,\n",
      " 'Gallia': 67,\n",
      " 'III': 3,\n",
      " 'Imperial': 80,\n",
      " 'January': 43,\n",
      " 'Japan': 24,\n",
      " 'Japanese': 10,\n",
      " 'Media.Vision': 37,\n",
      " 'Nameless': 61,\n",
      " 'PlayStation': 39,\n",
      " 'Portable': 40,\n",
      " 'Raven': 81,\n",
      " 'Released': 41,\n",
      " 'Second': 69,\n",
      " 'Sega': 35,\n",
      " 'Senjō': 4,\n",
      " 'Valkyria': 1,\n",
      " 'War': 71,\n",
      " 'a': 26,\n",
      " 'against': 79,\n",
      " 'and': 36,\n",
      " 'are': 77,\n",
      " 'as': 22,\n",
      " 'black': 75,\n",
      " 'by': 34,\n",
      " 'commonly': 19,\n",
      " 'developed': 33,\n",
      " 'during': 68,\n",
      " 'first': 58,\n",
      " 'follows': 59,\n",
      " 'for': 38,\n",
      " 'fusion': 49,\n",
      " 'game': 32,\n",
      " 'gameplay': 52,\n",
      " 'in': 42,\n",
      " 'is': 25,\n",
      " 'it': 45,\n",
      " 'its': 53,\n",
      " 'lit': 13,\n",
      " 'military': 63,\n",
      " 'nation': 66,\n",
      " 'no': 5,\n",
      " 'of': 15,\n",
      " 'operations': 76,\n",
      " 'outside': 23,\n",
      " 'parallel': 57,\n",
      " 'penal': 62,\n",
      " 'perform': 73,\n",
      " 'pitted': 78,\n",
      " 'playing': 30,\n",
      " 'predecessors': 54,\n",
      " 'real': 50,\n",
      " 'referred': 20,\n",
      " 'role': 28,\n",
      " 'runs': 56,\n",
      " 'same': 48,\n",
      " 'secret': 74,\n",
      " 'series': 47,\n",
      " 'serving': 65,\n",
      " 'story': 55,\n",
      " 'tactical': 27,\n",
      " 'the': 16,\n",
      " 'third': 46,\n",
      " 'time': 51,\n",
      " 'to': 21,\n",
      " 'unit': 64,\n",
      " 'video': 31,\n",
      " 'who': 72,\n",
      " '戦場のヴァルキュリア3': 11}\n",
      "['=',\n",
      " 'Valkyria',\n",
      " 'Chronicles',\n",
      " 'III',\n",
      " 'Senjō',\n",
      " 'no',\n",
      " '3',\n",
      " ':',\n",
      " '<unk>',\n",
      " '(',\n",
      " 'Japanese',\n",
      " '戦場のヴァルキュリア3',\n",
      " ',',\n",
      " 'lit',\n",
      " '.',\n",
      " 'of',\n",
      " 'the',\n",
      " 'Battlefield',\n",
      " ')',\n",
      " 'commonly',\n",
      " 'referred',\n",
      " 'to',\n",
      " 'as',\n",
      " 'outside',\n",
      " 'Japan',\n",
      " 'is',\n",
      " 'a',\n",
      " 'tactical',\n",
      " 'role',\n",
      " '@-@',\n",
      " 'playing',\n",
      " 'video',\n",
      " 'game',\n",
      " 'developed',\n",
      " 'by',\n",
      " 'Sega',\n",
      " 'and',\n",
      " 'Media.Vision',\n",
      " 'for',\n",
      " 'PlayStation',\n",
      " 'Portable',\n",
      " 'Released',\n",
      " 'in',\n",
      " 'January',\n",
      " '2011',\n",
      " 'it',\n",
      " 'third',\n",
      " 'series',\n",
      " 'same',\n",
      " 'fusion',\n",
      " 'real',\n",
      " 'time',\n",
      " 'gameplay',\n",
      " 'its',\n",
      " 'predecessors',\n",
      " 'story',\n",
      " 'runs',\n",
      " 'parallel',\n",
      " 'first',\n",
      " 'follows',\n",
      " '\"',\n",
      " 'Nameless',\n",
      " 'penal',\n",
      " 'military',\n",
      " 'unit',\n",
      " 'serving',\n",
      " 'nation',\n",
      " 'Gallia',\n",
      " 'during',\n",
      " 'Second',\n",
      " 'Europan',\n",
      " 'War',\n",
      " 'who',\n",
      " 'perform',\n",
      " 'secret',\n",
      " 'black',\n",
      " 'operations',\n",
      " 'are',\n",
      " 'pitted',\n",
      " 'against',\n",
      " 'Imperial',\n",
      " 'Raven']\n",
      "{'\"': 4,\n",
      " '(': 1,\n",
      " ')': 1,\n",
      " ',': 6,\n",
      " '.': 4,\n",
      " '2011': 1,\n",
      " '3': 2,\n",
      " ':': 2,\n",
      " '<unk>': 3,\n",
      " '=': 2,\n",
      " '@-@': 2,\n",
      " 'Battlefield': 1,\n",
      " 'Chronicles': 3,\n",
      " 'Europan': 1,\n",
      " 'Gallia': 1,\n",
      " 'III': 2,\n",
      " 'Imperial': 1,\n",
      " 'January': 1,\n",
      " 'Japan': 2,\n",
      " 'Japanese': 1,\n",
      " 'Media.Vision': 1,\n",
      " 'Nameless': 1,\n",
      " 'PlayStation': 1,\n",
      " 'Portable': 1,\n",
      " 'Raven': 1,\n",
      " 'Released': 1,\n",
      " 'Second': 1,\n",
      " 'Sega': 1,\n",
      " 'Senjō': 1,\n",
      " 'Valkyria': 5,\n",
      " 'War': 1,\n",
      " 'a': 2,\n",
      " 'against': 1,\n",
      " 'and': 4,\n",
      " 'are': 1,\n",
      " 'as': 2,\n",
      " 'black': 1,\n",
      " 'by': 1,\n",
      " 'commonly': 1,\n",
      " 'developed': 1,\n",
      " 'during': 1,\n",
      " 'first': 1,\n",
      " 'follows': 1,\n",
      " 'for': 1,\n",
      " 'fusion': 1,\n",
      " 'game': 3,\n",
      " 'gameplay': 1,\n",
      " 'in': 3,\n",
      " 'is': 2,\n",
      " 'it': 1,\n",
      " 'its': 1,\n",
      " 'lit': 1,\n",
      " 'military': 1,\n",
      " 'nation': 1,\n",
      " 'no': 1,\n",
      " 'of': 3,\n",
      " 'operations': 1,\n",
      " 'outside': 1,\n",
      " 'parallel': 1,\n",
      " 'penal': 1,\n",
      " 'perform': 1,\n",
      " 'pitted': 1,\n",
      " 'playing': 1,\n",
      " 'predecessors': 1,\n",
      " 'real': 1,\n",
      " 'referred': 1,\n",
      " 'role': 1,\n",
      " 'runs': 1,\n",
      " 'same': 1,\n",
      " 'secret': 1,\n",
      " 'series': 1,\n",
      " 'serving': 1,\n",
      " 'story': 1,\n",
      " 'tactical': 2,\n",
      " 'the': 11,\n",
      " 'third': 1,\n",
      " 'time': 1,\n",
      " 'to': 2,\n",
      " 'unit': 2,\n",
      " 'video': 1,\n",
      " 'who': 1,\n",
      " '戦場のヴァルキュリア3': 1}\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "# Let's take the four first lines of our training data:\n",
    "corpus = ''\n",
    "with open('./wikitext-2/train.txt', 'r', encoding=\"utf8\") as f:\n",
    "    for i in range(4):\n",
    "        corpus += f.readline()\n",
    "        \n",
    "# Create an empty Dictionary, separate and add all words. \n",
    "dictio = Dictionary()\n",
    "words = corpus.split()\n",
    "for word in words:\n",
    "    dictio.add_word(word)\n",
    "\n",
    "# Take a look at the objects created:\n",
    "pp.pprint(dictio.word2idx)\n",
    "pp.pprint(dictio.idx2word)\n",
    "pp.pprint(dictio.counter)\n",
    "pp.pprint(dictio.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        # We create an object Dictionary associated to Corpus\n",
    "        self.dictionary = Dictionary()\n",
    "        # We go through all files, adding all words to the dictionary\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "        \n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                tokens += len(words)\n",
    "        \n",
    "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
    "        ids = torch.LongTensor(tokens)\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idx = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[idx] = self.dictionary.word2idx[word]\n",
    "                    idx += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the corpus [wikitext-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/). While it's a small dataset, we will need to reduce it if we want to train a model on it without a gpu. With the 'small' version, on most computers, the model model should see one epoch of the data in less than 15 minutes on cpu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "data = './wikitext-2-small/'\n",
    "corpus = Corpus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383196\n",
      "19482\n",
      "19482\n",
      "torch.Size([275485])\n",
      "tensor([0, 1, 2, 3, 4, 1, 0])\n",
      "['<eos>', '=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']\n",
      "torch.Size([47945])\n",
      "tensor([    0,     1, 17642, 17643,     1,     0,     0])\n",
      "['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "#Examples and visualization\n",
    "print(corpus.dictionary.total)\n",
    "print(len(corpus.dictionary.idx2word))\n",
    "print(len(corpus.dictionary.word2idx))\n",
    "\n",
    "print(corpus.train.shape)\n",
    "print(corpus.train[0:7])\n",
    "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(7)])\n",
    "\n",
    "print(corpus.valid.shape)\n",
    "print(corpus.valid[0:7])\n",
    "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have data under a very long list of indexes: the text is as one sequence.\n",
    "Note that this is absolutely not the best way to proceed with large quantities of data (where we'll try not to store huge tensors in memory but read them from file as we go) !\n",
    "But here, we are looking for simplicity and efficiency with regards to computation time.\n",
    "That is why we will ignore sentence separations and treat the data as one long stream that we will cut arbitrarily as we need.\n",
    "\n",
    "\n",
    "The idea now is to create batches from this.\n",
    "With the alphabet being our data, we currently have the sequence:\n",
    "$$ \\left[ \\text{ a b c d e f g h i j k l m n o p q r s t u v w x y z } \\right] $$\n",
    "We want to reorganize it as independant batches that can be processed in parallel by the model !\n",
    "For instance, with the alphabet as the sequence and batch size 4, we'd get a batch of the 4 following sequences of the same length (6 letters):\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "\\text{a} & \\text{g} & \\text{m} & \\text{s} \\\\\n",
    "\\text{b} & \\text{h} & \\text{n} & \\text{t} \\\\\n",
    "\\text{c} & \\text{i} & \\text{o} & \\text{u} \\\\\n",
    "\\text{d} & \\text{j} & \\text{p} & \\text{v} \\\\\n",
    "\\text{e} & \\text{k} & \\text{q} & \\text{w} \\\\\n",
    "\\text{f} & \\text{l} & \\text{r} & \\text{x}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "with the last two elements being lost.\n",
    "Again, these columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing. The function ```batchify``` will allow us to reorganize the data as such, and if possible put it on the GPU. We need to cut the unnecessary elements, and put the data in the right shape.\n",
    "\n",
    "\n",
    "**Important**: You can notice that the data is ordered along the columns, which is unusual since it is the second dimension. To do so, you will need to transpose the matrix at some point. While it may not be how we usually organize batches, it will be useful when dealing with LSTMs and their particular organization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size, cuda = False):\n",
    "    # Cut the elements that are unnecessary - use the method 'narrow'\n",
    "    nbatch = data.size(0)//batch_size\n",
    "    data = data.narrow(0, 0, nbatch*batch_size)\n",
    "    \n",
    "    # Reorganize the data - use the method 'view'\n",
    "    data = data.view(-1, nbatch).t()\n",
    "    \n",
    "    # If we can use a GPU, let's tranfer the tensor to it\n",
    "    if cuda:\n",
    "        data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a way to divide our data into parallel batches. However, our network will not be able to process sequences of arbitrary length !\n",
    "We will then have to define a maximum length that sequences can have, and cut batches along their first dimension (which is the temporal dimension, as words are ordered this way)\n",
    "\n",
    "\n",
    "The function ```get_batch``` subdivides the source data into chunks of the appropriate length.\n",
    "It also separates the source data into the **input** and the **output** of the network. (Remember: we want to predict the next word, so the output is the input shifted by one step in the temporal axis).\n",
    "If ```source``` is equal to the example output of the batchify function, with a sequence length (seq_len) of 3, we'd get the following two variables:\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "\\text{a} & \\text{g} & \\text{m} & \\text{s} \\\\\n",
    "\\text{b} & \\text{h} & \\text{n} & \\text{t} \\\\\n",
    "\\text{c} & \\text{i} & \\text{o} & \\text{u} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "\\text{b} & \\text{h} & \\text{n} & \\text{t} \\\\\n",
    "\\text{c} & \\text{i} & \\text{o} & \\text{u} \\\\\n",
    "\\text{d} & \\text{j} & \\text{p} & \\text{v} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first variable contains the letters input to the network, while the second\n",
    "contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
    "Note that despite the name of the function, we are cutting the data in the\n",
    "temporal dimension, since we already divided data into batches in the previous\n",
    "function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, seq_len, evaluation=False):\n",
    "    # Deal with the possibility that there's not enough data left for a full sequence\n",
    "    upper = min(i+seq_len+1, source.size(0))\n",
    "    \n",
    "    # Take the input data - shift by one for the target data\n",
    "    data = source[i:upper-1]\n",
    "    target = source[i+1:upper]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2754, 100])\n",
      "torch.Size([11986, 4])\n"
     ]
    }
   ],
   "source": [
    "#Examples and visualization\n",
    "batch_size = 100\n",
    "eval_batch_size = 4\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,    10,    15,    91],\n",
      "        [    1,  3018,   735,    13],\n",
      "        [17642,   187,   766,   496]])\n",
      "tensor([[    1,  3018,   735,    13],\n",
      "        [17642,   187,   766,   496],\n",
      "        [17643,   827,   751,   131]])\n",
      "tensor([[17643,   827,   751,   131],\n",
      "        [    1,    19,  4659,  2200],\n",
      "        [    0,    17,  2466,    22]])\n",
      "tensor([[   1,   19, 4659, 2200],\n",
      "        [   0,   17, 2466,   22],\n",
      "        [   0, 3069,   39, 5521]])\n"
     ]
    }
   ],
   "source": [
    "#Examples and visualization\n",
    "input_words, target_words = get_batch(val_data, 0, 3)\n",
    "pp.pprint(input_words)\n",
    "pp.pprint(target_words)\n",
    "input_words, target_words = get_batch(val_data, 3, 3)\n",
    "pp.pprint(input_words)\n",
    "pp.pprint(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs expect inputs having 3 dimensions:\n",
    "- The first dimension is the temporal dimension, along which we (in our case) have the different words\n",
    "- The second dimension is the batch dimension, along which we stack the independant batches\n",
    "- The third dimension is the feature dimension, along which are the features of the vector representing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy example of LSTM: \n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our toy case, we have inputs and outputs containing 3 features (third dimension !)\n",
    "We created a sequence of 5 different inputs (first dimension !)\n",
    "We don't use batch (the second dimension will have one lement)\n",
    "\n",
    "\n",
    "We need an initial hidden state, of the right sizes for dimension 2/3, but with only one temporal element:\n",
    "Here, it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we create a tuple of two tensors ? Because we use LSTMs: remember that they use two sets of weights,\n",
    "and two hidden states (Hidden state, and Cell state).\n",
    "If you don't remember, read https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "If we used a classic RNN, we would simply have ```hidden = torch.randn(1, 1, 3)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The naive way of applying a lstm to inputs is to apply it one step at a time, and loop through the sequence\n",
    "for i in inputs:\n",
    "    # After each step, hidden contains the hidden states (remember, it's a tuple of two states).\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can do the entire sequence all at once.\n",
    "The first value returned by LSTM is all of the Hidden states throughout the sequence.\n",
    "The second is just the most recent Hidden state and Cell state (you can compare the values)\n",
    "The reason for this is that:\n",
    "- ```out``` will give you access to all hidden states in the sequence, for each temporal step\n",
    "- ```hidden``` will allow you to continue the sequence and backpropagate later, with another sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1802, -0.1396,  0.0871]],\n",
      "\n",
      "        [[ 0.3534, -0.0703,  0.2642]],\n",
      "\n",
      "        [[ 0.0751, -0.1487,  0.3096]],\n",
      "\n",
      "        [[ 0.0006, -0.0440,  0.2825]],\n",
      "\n",
      "        [[ 0.0354, -0.2225,  0.1823]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[ 0.0354, -0.2225,  0.1823]]], grad_fn=<StackBackward>),\n",
      " tensor([[[ 0.2306, -0.3665,  0.7337]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # Re-initialize\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "pp.pprint(out)\n",
    "pp.pprint(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our own LSTM Model\n",
    "\n",
    "In Pytorch, models are usually implemented as custom ```nn.Module``` subclass:\n",
    "- We need to redefine the ```__init__``` method, which creates the object\n",
    "- We also need to redefine the ```forward``` method, which transform the input into outputs\n",
    "- We can also add any method that we need: here, in order to initiate weights in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Create a dropout object to use on layers for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create an encoder - which is an embedding layer\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        \n",
    "        # Create the LSTM layers - find out how to stack them !\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers)\n",
    "        \n",
    "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
    "        # (Note that the softmax application function will be applied out of the model)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        \n",
    "        # Initialize non-reccurent weights \n",
    "        self.init_weights()\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialize the encoder and decoder weights with the uniform distribution\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
    "        weight = next(self.parameters())\n",
    "        \n",
    "        hidden = (torch.zeros(self.nlayers, batch_size, self.nhid),\n",
    "                  torch.zeros(self.nlayers, batch_size, self.nhid))\n",
    "        \n",
    "        return hidden    \n",
    "\n",
    "    def forward(self, input, hidden, return_h=False):\n",
    "        # Process the input\n",
    "        encoded = self.encoder(input)\n",
    "        \n",
    "        # Apply the LSTMs\n",
    "        outputs, hidden = self.lstm(encoded, hidden)\n",
    "        \n",
    "        # Decode into scores\n",
    "        outputs = self.dropout(outputs)\n",
    "        decoded = self.decoder(outputs)\n",
    "        \n",
    "        if not return_h:\n",
    "            hidden = None\n",
    "        \n",
    "        return decoded, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# If you have Cuda installed and a GPU available\n",
    "cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    if not cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably choose cuda = True\")\n",
    "        \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 200\n",
    "layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "vocab_size = len(corpus.dictionary.idx2word)\n",
    "model = LSTMModel(ntoken=vocab_size, \n",
    "                  ninp=embedding_size, \n",
    "                  nhid=hidden_size, nlayers=layers, \n",
    "                  dropout=dropout)\n",
    "model = model.to(device)\n",
    "params = list(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10.0\n",
    "optimizer = 'sgd'\n",
    "wdecay = 1.2e-6\n",
    "# For gradient clipping\n",
    "clip = 0.25\n",
    "\n",
    "# Create the optimizer\n",
    "if optimizer == 'sgd':\n",
    "    optim = torch.optim.SGD(model.parameters(), \n",
    "                      lr=lr,\n",
    "                      weight_decay=wdecay)\n",
    "elif optimizer == 'adam':\n",
    "    optim = torch.optim.Adam(model.parameters(), \n",
    "                           lr=lr,\n",
    "                           weight_decay=wdecay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about gradient propagation:\n",
    "\n",
    "We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
    "the next call to LSTM. This way, we can back-propagate the gradient for as long as we want.\n",
    "However, this puts a huge strain on the memory used by the model, since it implies retaining\n",
    "a always-growing number of tensors of gradients in the cache.\n",
    "\n",
    "\n",
    "We decide to not backpropagate through time beyond the current sequence ! \n",
    "We use a specific function to **cut the 'hidden/state cell' states from their previous dependencies**\n",
    "before using them to initialize the next call to the LSTM.\n",
    "This is done with the ```.detach()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other global parameters\n",
    "epochs = 10\n",
    "seq_len = 30\n",
    "log_interval = 1\n",
    "save = 'model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything necessary to define the training loop. \n",
    "Note that ```nn.Module``` objects override the ```__call__``` operator so you can call them like functions, which will have the same effect as calling their ```.forward()``` method:\n",
    "\n",
    "\n",
    "In practice, we use ```outputs = model(inputs)``` rather than ``` outputs = model.forward(inputs)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    # Initialize the hidden/cell state\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
    "        # Get the input/target data\n",
    "        inputs, targets = get_batch(train_data, i, seq_len)\n",
    "        \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        # Do the training loop: careful, look into the documentation for the criterion CrossEntropyLoss()\n",
    "        outputs, hidden = model(inputs, hidden, return_h=True)\n",
    "        loss = criterion(outputs.permute(1,2,0), targets.permute(1,0))\n",
    "        \n",
    "        # Do the gradient clipping with the function torch.nn.utils.clip_grad_norm_, then the optimization step  \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # We use .data to only accumulate the loss, and not keep track of the gradient too\n",
    "        total_loss += loss.data.item()\n",
    "        \n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // seq_len, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    # Initialize the hidden/cell state\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
    "            inputs, targets = get_batch(data_source, i, seq_len)\n",
    "            outputs, _ = model(inputs, hidden, return_h=False)\n",
    "            loss = criterion(outputs.permute(1,2,0), targets.permute(1,0))\n",
    "            total_loss += loss.data.item()\n",
    "            \n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/   91 batches | lr 10.00 | ms/batch 8050.45 | loss 19.57 | ppl 315394259.03\n",
      "| epoch   1 |     2/   91 batches | lr 10.00 | ms/batch 3925.64 | loss  9.38 | ppl 11805.78\n",
      "| epoch   1 |     3/   91 batches | lr 10.00 | ms/batch 3780.62 | loss  8.45 | ppl  4672.72\n",
      "| epoch   1 |     4/   91 batches | lr 10.00 | ms/batch 3817.53 | loss  9.21 | ppl 10011.75\n",
      "| epoch   1 |     5/   91 batches | lr 10.00 | ms/batch 3914.31 | loss  8.55 | ppl  5144.80\n",
      "| epoch   1 |     6/   91 batches | lr 10.00 | ms/batch 3843.11 | loss  9.02 | ppl  8247.54\n",
      "| epoch   1 |     7/   91 batches | lr 10.00 | ms/batch 3933.26 | loss  8.77 | ppl  6447.47\n",
      "| epoch   1 |     8/   91 batches | lr 10.00 | ms/batch 3948.12 | loss  9.28 | ppl 10739.71\n",
      "| epoch   1 |     9/   91 batches | lr 10.00 | ms/batch 4685.29 | loss  8.69 | ppl  5970.19\n",
      "| epoch   1 |    10/   91 batches | lr 10.00 | ms/batch 4095.87 | loss  8.52 | ppl  5021.65\n",
      "| epoch   1 |    11/   91 batches | lr 10.00 | ms/batch 4408.05 | loss  9.46 | ppl 12802.47\n",
      "| epoch   1 |    12/   91 batches | lr 10.00 | ms/batch 4170.75 | loss  9.20 | ppl  9869.76\n",
      "| epoch   1 |    13/   91 batches | lr 10.00 | ms/batch 4520.15 | loss  8.83 | ppl  6820.85\n",
      "| epoch   1 |    14/   91 batches | lr 10.00 | ms/batch 4130.71 | loss  8.45 | ppl  4652.41\n",
      "| epoch   1 |    15/   91 batches | lr 10.00 | ms/batch 4115.22 | loss  8.20 | ppl  3633.69\n",
      "| epoch   1 |    16/   91 batches | lr 10.00 | ms/batch 3949.65 | loss  8.39 | ppl  4405.18\n",
      "| epoch   1 |    17/   91 batches | lr 10.00 | ms/batch 4022.34 | loss  8.70 | ppl  5988.80\n",
      "| epoch   1 |    18/   91 batches | lr 10.00 | ms/batch 4310.55 | loss  9.20 | ppl  9898.04\n",
      "| epoch   1 |    19/   91 batches | lr 10.00 | ms/batch 3998.72 | loss  8.75 | ppl  6284.80\n",
      "| epoch   1 |    20/   91 batches | lr 10.00 | ms/batch 4028.19 | loss  8.75 | ppl  6282.79\n",
      "| epoch   1 |    21/   91 batches | lr 10.00 | ms/batch 4031.49 | loss  8.44 | ppl  4617.76\n",
      "| epoch   1 |    22/   91 batches | lr 10.00 | ms/batch 4101.57 | loss  8.32 | ppl  4088.55\n",
      "| epoch   1 |    23/   91 batches | lr 10.00 | ms/batch 3989.97 | loss  8.28 | ppl  3932.90\n",
      "| epoch   1 |    24/   91 batches | lr 10.00 | ms/batch 4080.77 | loss  7.92 | ppl  2756.29\n",
      "| epoch   1 |    25/   91 batches | lr 10.00 | ms/batch 4072.11 | loss  7.90 | ppl  2706.28\n",
      "| epoch   1 |    26/   91 batches | lr 10.00 | ms/batch 3906.65 | loss  7.67 | ppl  2142.05\n",
      "| epoch   1 |    27/   91 batches | lr 10.00 | ms/batch 3872.71 | loss  8.06 | ppl  3177.30\n",
      "| epoch   1 |    28/   91 batches | lr 10.00 | ms/batch 3982.36 | loss  8.31 | ppl  4065.45\n",
      "| epoch   1 |    29/   91 batches | lr 10.00 | ms/batch 3936.15 | loss  8.04 | ppl  3102.68\n",
      "| epoch   1 |    30/   91 batches | lr 10.00 | ms/batch 3829.26 | loss  7.80 | ppl  2443.15\n",
      "| epoch   1 |    31/   91 batches | lr 10.00 | ms/batch 4051.02 | loss  7.70 | ppl  2208.73\n",
      "| epoch   1 |    32/   91 batches | lr 10.00 | ms/batch 4397.73 | loss  7.59 | ppl  1973.65\n",
      "| epoch   1 |    33/   91 batches | lr 10.00 | ms/batch 4005.14 | loss  7.94 | ppl  2804.91\n",
      "| epoch   1 |    34/   91 batches | lr 10.00 | ms/batch 3974.82 | loss  8.36 | ppl  4281.46\n",
      "| epoch   1 |    35/   91 batches | lr 10.00 | ms/batch 6903.33 | loss  8.10 | ppl  3290.67\n",
      "| epoch   1 |    36/   91 batches | lr 10.00 | ms/batch 4023.85 | loss  8.02 | ppl  3049.52\n",
      "| epoch   1 |    37/   91 batches | lr 10.00 | ms/batch 4338.85 | loss  7.93 | ppl  2786.78\n",
      "| epoch   1 |    38/   91 batches | lr 10.00 | ms/batch 3990.04 | loss  7.73 | ppl  2283.08\n",
      "| epoch   1 |    39/   91 batches | lr 10.00 | ms/batch 4132.71 | loss  7.68 | ppl  2167.11\n",
      "| epoch   1 |    40/   91 batches | lr 10.00 | ms/batch 4184.93 | loss  7.56 | ppl  1919.11\n",
      "| epoch   1 |    41/   91 batches | lr 10.00 | ms/batch 4741.66 | loss  7.67 | ppl  2147.62\n",
      "| epoch   1 |    42/   91 batches | lr 10.00 | ms/batch 3912.64 | loss  8.02 | ppl  3032.14\n",
      "| epoch   1 |    43/   91 batches | lr 10.00 | ms/batch 4056.80 | loss  7.69 | ppl  2189.48\n",
      "| epoch   1 |    44/   91 batches | lr 10.00 | ms/batch 3976.36 | loss  7.66 | ppl  2126.55\n",
      "| epoch   1 |    45/   91 batches | lr 10.00 | ms/batch 4918.03 | loss  7.70 | ppl  2205.68\n",
      "| epoch   1 |    46/   91 batches | lr 10.00 | ms/batch 4612.14 | loss  7.57 | ppl  1948.23\n",
      "| epoch   1 |    47/   91 batches | lr 10.00 | ms/batch 5011.95 | loss  7.54 | ppl  1881.51\n",
      "| epoch   1 |    48/   91 batches | lr 10.00 | ms/batch 4457.84 | loss  7.36 | ppl  1566.51\n",
      "| epoch   1 |    49/   91 batches | lr 10.00 | ms/batch 4681.20 | loss  7.44 | ppl  1702.12\n",
      "| epoch   1 |    50/   91 batches | lr 10.00 | ms/batch 3980.21 | loss  7.43 | ppl  1677.70\n",
      "| epoch   1 |    51/   91 batches | lr 10.00 | ms/batch 3942.70 | loss  7.45 | ppl  1715.49\n",
      "| epoch   1 |    52/   91 batches | lr 10.00 | ms/batch 3846.54 | loss  7.40 | ppl  1637.24\n",
      "| epoch   1 |    53/   91 batches | lr 10.00 | ms/batch 3989.34 | loss  7.36 | ppl  1576.80\n",
      "| epoch   1 |    54/   91 batches | lr 10.00 | ms/batch 3909.62 | loss  7.37 | ppl  1591.11\n",
      "| epoch   1 |    55/   91 batches | lr 10.00 | ms/batch 4071.50 | loss  7.48 | ppl  1772.14\n",
      "| epoch   1 |    56/   91 batches | lr 10.00 | ms/batch 4174.92 | loss  7.58 | ppl  1957.60\n",
      "| epoch   1 |    57/   91 batches | lr 10.00 | ms/batch 4560.67 | loss  7.40 | ppl  1637.18\n",
      "| epoch   1 |    58/   91 batches | lr 10.00 | ms/batch 3896.76 | loss  7.43 | ppl  1685.83\n",
      "| epoch   1 |    59/   91 batches | lr 10.00 | ms/batch 3933.48 | loss  7.25 | ppl  1406.34\n",
      "| epoch   1 |    60/   91 batches | lr 10.00 | ms/batch 3914.31 | loss  7.36 | ppl  1566.76\n",
      "| epoch   1 |    61/   91 batches | lr 10.00 | ms/batch 3837.39 | loss  7.42 | ppl  1667.38\n",
      "| epoch   1 |    62/   91 batches | lr 10.00 | ms/batch 4039.72 | loss  7.36 | ppl  1572.75\n",
      "| epoch   1 |    63/   91 batches | lr 10.00 | ms/batch 4353.81 | loss  7.36 | ppl  1572.12\n",
      "| epoch   1 |    64/   91 batches | lr 10.00 | ms/batch 4082.43 | loss  7.34 | ppl  1534.42\n",
      "| epoch   1 |    65/   91 batches | lr 10.00 | ms/batch 3846.63 | loss  7.40 | ppl  1642.74\n",
      "| epoch   1 |    66/   91 batches | lr 10.00 | ms/batch 4013.96 | loss  7.33 | ppl  1518.74\n",
      "| epoch   1 |    67/   91 batches | lr 10.00 | ms/batch 4033.16 | loss  7.28 | ppl  1451.38\n",
      "| epoch   1 |    68/   91 batches | lr 10.00 | ms/batch 4238.72 | loss  7.32 | ppl  1508.32\n",
      "| epoch   1 |    69/   91 batches | lr 10.00 | ms/batch 3866.86 | loss  7.36 | ppl  1578.96\n",
      "| epoch   1 |    70/   91 batches | lr 10.00 | ms/batch 4016.80 | loss  7.35 | ppl  1548.57\n",
      "| epoch   1 |    71/   91 batches | lr 10.00 | ms/batch 4392.45 | loss  7.32 | ppl  1515.63\n",
      "| epoch   1 |    72/   91 batches | lr 10.00 | ms/batch 4258.19 | loss  7.35 | ppl  1558.47\n",
      "| epoch   1 |    73/   91 batches | lr 10.00 | ms/batch 4933.00 | loss  7.34 | ppl  1547.41\n",
      "| epoch   1 |    74/   91 batches | lr 10.00 | ms/batch 4803.90 | loss  7.48 | ppl  1772.31\n",
      "| epoch   1 |    75/   91 batches | lr 10.00 | ms/batch 4676.88 | loss  7.37 | ppl  1587.84\n",
      "| epoch   1 |    76/   91 batches | lr 10.00 | ms/batch 4191.92 | loss  7.24 | ppl  1397.77\n",
      "| epoch   1 |    77/   91 batches | lr 10.00 | ms/batch 4115.60 | loss  7.21 | ppl  1355.41\n",
      "| epoch   1 |    78/   91 batches | lr 10.00 | ms/batch 4069.80 | loss  7.35 | ppl  1558.41\n",
      "| epoch   1 |    79/   91 batches | lr 10.00 | ms/batch 4008.88 | loss  7.41 | ppl  1658.25\n",
      "| epoch   1 |    80/   91 batches | lr 10.00 | ms/batch 4040.70 | loss  7.33 | ppl  1530.70\n",
      "| epoch   1 |    81/   91 batches | lr 10.00 | ms/batch 4001.36 | loss  7.44 | ppl  1709.86\n",
      "| epoch   1 |    82/   91 batches | lr 10.00 | ms/batch 4056.24 | loss  7.33 | ppl  1519.76\n",
      "| epoch   1 |    83/   91 batches | lr 10.00 | ms/batch 4015.47 | loss  7.32 | ppl  1516.38\n",
      "| epoch   1 |    84/   91 batches | lr 10.00 | ms/batch 4500.74 | loss  7.39 | ppl  1627.17\n",
      "| epoch   1 |    85/   91 batches | lr 10.00 | ms/batch 3982.04 | loss  7.20 | ppl  1337.93\n",
      "| epoch   1 |    86/   91 batches | lr 10.00 | ms/batch 4027.85 | loss  7.26 | ppl  1425.58\n",
      "| epoch   1 |    87/   91 batches | lr 10.00 | ms/batch 4212.96 | loss  7.37 | ppl  1590.71\n",
      "| epoch   1 |    88/   91 batches | lr 10.00 | ms/batch 4429.31 | loss  7.28 | ppl  1456.29\n",
      "| epoch   1 |    89/   91 batches | lr 10.00 | ms/batch 3912.47 | loss  7.26 | ppl  1422.20\n",
      "| epoch   1 |    90/   91 batches | lr 10.00 | ms/batch 4057.41 | loss  7.29 | ppl  1467.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    91/   91 batches | lr 10.00 | ms/batch 2959.24 | loss  7.23 | ppl  1383.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 409.76s | valid loss  0.24 | valid ppl     1.27\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haiyen Vu\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type LSTMModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |     1/   91 batches | lr 10.00 | ms/batch 8356.94 | loss 14.55 | ppl 2078198.25\n",
      "| epoch   2 |     2/   91 batches | lr 10.00 | ms/batch 3994.66 | loss  7.29 | ppl  1472.68\n",
      "| epoch   2 |     3/   91 batches | lr 10.00 | ms/batch 4015.95 | loss  7.17 | ppl  1299.39\n",
      "| epoch   2 |     4/   91 batches | lr 10.00 | ms/batch 4690.47 | loss  7.17 | ppl  1302.47\n",
      "| epoch   2 |     5/   91 batches | lr 10.00 | ms/batch 4802.64 | loss  7.17 | ppl  1301.04\n",
      "| epoch   2 |     6/   91 batches | lr 10.00 | ms/batch 4173.76 | loss  7.30 | ppl  1474.34\n",
      "| epoch   2 |     7/   91 batches | lr 10.00 | ms/batch 4064.74 | loss  7.15 | ppl  1274.12\n",
      "| epoch   2 |     8/   91 batches | lr 10.00 | ms/batch 4481.30 | loss  7.20 | ppl  1334.08\n",
      "| epoch   2 |     9/   91 batches | lr 10.00 | ms/batch 4318.31 | loss  7.19 | ppl  1332.52\n",
      "| epoch   2 |    10/   91 batches | lr 10.00 | ms/batch 4158.11 | loss  7.17 | ppl  1295.71\n",
      "| epoch   2 |    11/   91 batches | lr 10.00 | ms/batch 4148.88 | loss  7.27 | ppl  1435.60\n",
      "| epoch   2 |    12/   91 batches | lr 10.00 | ms/batch 4487.69 | loss  7.46 | ppl  1738.34\n",
      "| epoch   2 |    13/   91 batches | lr 10.00 | ms/batch 5014.83 | loss  7.21 | ppl  1348.48\n",
      "| epoch   2 |    14/   91 batches | lr 10.00 | ms/batch 4142.64 | loss  7.25 | ppl  1409.81\n",
      "| epoch   2 |    15/   91 batches | lr 10.00 | ms/batch 4010.68 | loss  7.28 | ppl  1444.95\n",
      "| epoch   2 |    16/   91 batches | lr 10.00 | ms/batch 4091.16 | loss  7.17 | ppl  1298.34\n",
      "| epoch   2 |    17/   91 batches | lr 10.00 | ms/batch 3964.74 | loss  7.19 | ppl  1319.53\n",
      "| epoch   2 |    18/   91 batches | lr 10.00 | ms/batch 4012.04 | loss  7.20 | ppl  1342.69\n",
      "| epoch   2 |    19/   91 batches | lr 10.00 | ms/batch 4129.51 | loss  7.26 | ppl  1423.74\n",
      "| epoch   2 |    20/   91 batches | lr 10.00 | ms/batch 4101.57 | loss  7.27 | ppl  1438.05\n",
      "| epoch   2 |    21/   91 batches | lr 10.00 | ms/batch 4201.88 | loss  7.24 | ppl  1389.24\n",
      "| epoch   2 |    22/   91 batches | lr 10.00 | ms/batch 3998.93 | loss  7.18 | ppl  1315.57\n",
      "| epoch   2 |    23/   91 batches | lr 10.00 | ms/batch 4463.68 | loss  7.21 | ppl  1351.21\n",
      "| epoch   2 |    24/   91 batches | lr 10.00 | ms/batch 5258.84 | loss  6.99 | ppl  1081.74\n",
      "| epoch   2 |    25/   91 batches | lr 10.00 | ms/batch 3953.60 | loss  7.19 | ppl  1330.43\n",
      "| epoch   2 |    26/   91 batches | lr 10.00 | ms/batch 4014.12 | loss  7.36 | ppl  1578.85\n",
      "| epoch   2 |    27/   91 batches | lr 10.00 | ms/batch 4068.74 | loss  7.32 | ppl  1504.55\n",
      "| epoch   2 |    28/   91 batches | lr 10.00 | ms/batch 4020.06 | loss  7.13 | ppl  1254.65\n",
      "| epoch   2 |    29/   91 batches | lr 10.00 | ms/batch 4085.53 | loss  7.12 | ppl  1238.52\n",
      "| epoch   2 |    30/   91 batches | lr 10.00 | ms/batch 4027.76 | loss  7.24 | ppl  1388.11\n",
      "| epoch   2 |    31/   91 batches | lr 10.00 | ms/batch 5459.55 | loss  7.22 | ppl  1373.21\n",
      "| epoch   2 |    32/   91 batches | lr 10.00 | ms/batch 4338.16 | loss  7.09 | ppl  1198.23\n",
      "| epoch   2 |    33/   91 batches | lr 10.00 | ms/batch 4804.86 | loss  7.13 | ppl  1252.45\n",
      "| epoch   2 |    34/   91 batches | lr 10.00 | ms/batch 6003.88 | loss  7.12 | ppl  1234.87\n",
      "| epoch   2 |    35/   91 batches | lr 10.00 | ms/batch 5303.45 | loss  7.22 | ppl  1370.63\n",
      "| epoch   2 |    36/   91 batches | lr 10.00 | ms/batch 4261.72 | loss  7.14 | ppl  1260.17\n",
      "| epoch   2 |    37/   91 batches | lr 10.00 | ms/batch 4290.86 | loss  7.08 | ppl  1182.39\n",
      "| epoch   2 |    38/   91 batches | lr 10.00 | ms/batch 4368.54 | loss  7.00 | ppl  1096.20\n",
      "| epoch   2 |    39/   91 batches | lr 10.00 | ms/batch 4129.60 | loss  7.08 | ppl  1187.62\n",
      "| epoch   2 |    40/   91 batches | lr 10.00 | ms/batch 3994.30 | loss  7.14 | ppl  1266.97\n",
      "| epoch   2 |    41/   91 batches | lr 10.00 | ms/batch 4009.92 | loss  7.03 | ppl  1125.26\n",
      "| epoch   2 |    42/   91 batches | lr 10.00 | ms/batch 4057.85 | loss  7.05 | ppl  1154.50\n",
      "| epoch   2 |    43/   91 batches | lr 10.00 | ms/batch 4000.75 | loss  7.01 | ppl  1104.48\n",
      "| epoch   2 |    44/   91 batches | lr 10.00 | ms/batch 4133.77 | loss  7.24 | ppl  1395.49\n",
      "| epoch   2 |    45/   91 batches | lr 10.00 | ms/batch 3982.45 | loss  7.31 | ppl  1499.15\n",
      "| epoch   2 |    46/   91 batches | lr 10.00 | ms/batch 3987.04 | loss  7.06 | ppl  1165.82\n",
      "| epoch   2 |    47/   91 batches | lr 10.00 | ms/batch 4042.88 | loss  7.08 | ppl  1191.57\n",
      "| epoch   2 |    48/   91 batches | lr 10.00 | ms/batch 4053.56 | loss  6.99 | ppl  1085.87\n",
      "| epoch   2 |    49/   91 batches | lr 10.00 | ms/batch 4031.10 | loss  7.07 | ppl  1180.26\n",
      "| epoch   2 |    50/   91 batches | lr 10.00 | ms/batch 4049.25 | loss  7.08 | ppl  1193.08\n",
      "| epoch   2 |    51/   91 batches | lr 10.00 | ms/batch 4001.70 | loss  7.12 | ppl  1233.81\n",
      "| epoch   2 |    52/   91 batches | lr 10.00 | ms/batch 4084.48 | loss  7.13 | ppl  1246.16\n",
      "| epoch   2 |    53/   91 batches | lr 10.00 | ms/batch 3888.74 | loss  7.02 | ppl  1118.39\n",
      "| epoch   2 |    54/   91 batches | lr 10.00 | ms/batch 3983.10 | loss  7.00 | ppl  1094.84\n",
      "| epoch   2 |    55/   91 batches | lr 10.00 | ms/batch 4066.07 | loss  7.12 | ppl  1238.63\n",
      "| epoch   2 |    56/   91 batches | lr 10.00 | ms/batch 4001.01 | loss  7.33 | ppl  1529.38\n",
      "| epoch   2 |    57/   91 batches | lr 10.00 | ms/batch 3884.33 | loss  7.12 | ppl  1234.35\n",
      "| epoch   2 |    58/   91 batches | lr 10.00 | ms/batch 3873.54 | loss  7.14 | ppl  1264.90\n",
      "| epoch   2 |    59/   91 batches | lr 10.00 | ms/batch 4024.23 | loss  6.91 | ppl  1002.34\n",
      "| epoch   2 |    60/   91 batches | lr 10.00 | ms/batch 3991.24 | loss  7.00 | ppl  1098.92\n",
      "| epoch   2 |    61/   91 batches | lr 10.00 | ms/batch 4024.36 | loss  7.16 | ppl  1284.90\n",
      "| epoch   2 |    62/   91 batches | lr 10.00 | ms/batch 4102.66 | loss  7.06 | ppl  1165.48\n",
      "| epoch   2 |    63/   91 batches | lr 10.00 | ms/batch 4048.07 | loss  7.04 | ppl  1140.19\n",
      "| epoch   2 |    64/   91 batches | lr 10.00 | ms/batch 4035.06 | loss  7.05 | ppl  1152.16\n",
      "| epoch   2 |    65/   91 batches | lr 10.00 | ms/batch 4018.03 | loss  7.11 | ppl  1222.58\n",
      "| epoch   2 |    66/   91 batches | lr 10.00 | ms/batch 3895.67 | loss  6.96 | ppl  1058.18\n",
      "| epoch   2 |    67/   91 batches | lr 10.00 | ms/batch 4019.60 | loss  7.04 | ppl  1138.40\n",
      "| epoch   2 |    68/   91 batches | lr 10.00 | ms/batch 4070.72 | loss  6.96 | ppl  1055.50\n",
      "| epoch   2 |    69/   91 batches | lr 10.00 | ms/batch 4156.64 | loss  7.01 | ppl  1105.02\n",
      "| epoch   2 |    70/   91 batches | lr 10.00 | ms/batch 4018.95 | loss  7.03 | ppl  1131.49\n",
      "| epoch   2 |    71/   91 batches | lr 10.00 | ms/batch 3977.48 | loss  7.11 | ppl  1222.66\n",
      "| epoch   2 |    72/   91 batches | lr 10.00 | ms/batch 4540.48 | loss  7.14 | ppl  1267.68\n",
      "| epoch   2 |    73/   91 batches | lr 10.00 | ms/batch 4038.85 | loss  7.08 | ppl  1188.54\n",
      "| epoch   2 |    74/   91 batches | lr 10.00 | ms/batch 3891.50 | loss  7.25 | ppl  1403.24\n",
      "| epoch   2 |    75/   91 batches | lr 10.00 | ms/batch 4497.22 | loss  7.19 | ppl  1326.93\n",
      "| epoch   2 |    76/   91 batches | lr 10.00 | ms/batch 4030.87 | loss  6.90 | ppl   996.87\n",
      "| epoch   2 |    77/   91 batches | lr 10.00 | ms/batch 5398.59 | loss  6.91 | ppl   999.87\n",
      "| epoch   2 |    78/   91 batches | lr 10.00 | ms/batch 4183.74 | loss  7.17 | ppl  1300.72\n",
      "| epoch   2 |    79/   91 batches | lr 10.00 | ms/batch 4002.99 | loss  7.14 | ppl  1265.57\n",
      "| epoch   2 |    80/   91 batches | lr 10.00 | ms/batch 4187.96 | loss  7.11 | ppl  1218.42\n",
      "| epoch   2 |    81/   91 batches | lr 10.00 | ms/batch 4036.23 | loss  7.22 | ppl  1361.84\n",
      "| epoch   2 |    82/   91 batches | lr 10.00 | ms/batch 4083.99 | loss  7.05 | ppl  1151.91\n",
      "| epoch   2 |    83/   91 batches | lr 10.00 | ms/batch 4004.13 | loss  7.06 | ppl  1168.52\n",
      "| epoch   2 |    84/   91 batches | lr 10.00 | ms/batch 4008.07 | loss  7.21 | ppl  1359.64\n",
      "| epoch   2 |    85/   91 batches | lr 10.00 | ms/batch 4098.08 | loss  7.01 | ppl  1109.28\n",
      "| epoch   2 |    86/   91 batches | lr 10.00 | ms/batch 3970.82 | loss  6.93 | ppl  1024.40\n",
      "| epoch   2 |    87/   91 batches | lr 10.00 | ms/batch 4120.85 | loss  6.92 | ppl  1010.22\n",
      "| epoch   2 |    88/   91 batches | lr 10.00 | ms/batch 4162.76 | loss  7.10 | ppl  1206.43\n",
      "| epoch   2 |    89/   91 batches | lr 10.00 | ms/batch 4102.98 | loss  7.44 | ppl  1695.92\n",
      "| epoch   2 |    90/   91 batches | lr 10.00 | ms/batch 4072.64 | loss  7.22 | ppl  1372.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |    91/   91 batches | lr 10.00 | ms/batch 3020.95 | loss  7.09 | ppl  1201.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 417.27s | valid loss  0.23 | valid ppl     1.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |     1/   91 batches | lr 10.00 | ms/batch 8793.42 | loss 13.95 | ppl 1143161.15\n",
      "| epoch   3 |     2/   91 batches | lr 10.00 | ms/batch 4016.78 | loss  7.00 | ppl  1096.20\n",
      "| epoch   3 |     3/   91 batches | lr 10.00 | ms/batch 4629.12 | loss  6.82 | ppl   913.89\n",
      "| epoch   3 |     4/   91 batches | lr 10.00 | ms/batch 4455.66 | loss  6.95 | ppl  1041.83\n",
      "| epoch   3 |     5/   91 batches | lr 10.00 | ms/batch 4672.42 | loss  6.98 | ppl  1079.13\n",
      "| epoch   3 |     6/   91 batches | lr 10.00 | ms/batch 4720.16 | loss  7.05 | ppl  1158.23\n",
      "| epoch   3 |     7/   91 batches | lr 10.00 | ms/batch 5036.17 | loss  7.05 | ppl  1151.77\n",
      "| epoch   3 |     8/   91 batches | lr 10.00 | ms/batch 5057.35 | loss  6.94 | ppl  1033.86\n",
      "| epoch   3 |     9/   91 batches | lr 10.00 | ms/batch 4367.56 | loss  6.96 | ppl  1055.49\n",
      "| epoch   3 |    10/   91 batches | lr 10.00 | ms/batch 4109.93 | loss  6.97 | ppl  1062.65\n",
      "| epoch   3 |    11/   91 batches | lr 10.00 | ms/batch 3978.68 | loss  7.04 | ppl  1135.79\n",
      "| epoch   3 |    12/   91 batches | lr 10.00 | ms/batch 4533.73 | loss  6.97 | ppl  1065.86\n",
      "| epoch   3 |    13/   91 batches | lr 10.00 | ms/batch 4426.30 | loss  6.94 | ppl  1027.87\n",
      "| epoch   3 |    14/   91 batches | lr 10.00 | ms/batch 4060.32 | loss  7.06 | ppl  1165.06\n",
      "| epoch   3 |    15/   91 batches | lr 10.00 | ms/batch 4005.61 | loss  6.98 | ppl  1072.89\n",
      "| epoch   3 |    16/   91 batches | lr 10.00 | ms/batch 4035.63 | loss  6.98 | ppl  1070.62\n",
      "| epoch   3 |    17/   91 batches | lr 10.00 | ms/batch 4079.02 | loss  6.93 | ppl  1020.89\n",
      "| epoch   3 |    18/   91 batches | lr 10.00 | ms/batch 3852.49 | loss  7.07 | ppl  1177.54\n",
      "| epoch   3 |    19/   91 batches | lr 10.00 | ms/batch 4009.07 | loss  6.87 | ppl   962.51\n",
      "| epoch   3 |    20/   91 batches | lr 10.00 | ms/batch 4142.90 | loss  6.96 | ppl  1050.60\n",
      "| epoch   3 |    21/   91 batches | lr 10.00 | ms/batch 3984.26 | loss  6.92 | ppl  1014.90\n",
      "| epoch   3 |    22/   91 batches | lr 10.00 | ms/batch 4078.80 | loss  7.09 | ppl  1197.67\n",
      "| epoch   3 |    23/   91 batches | lr 10.00 | ms/batch 4065.73 | loss  6.99 | ppl  1085.25\n",
      "| epoch   3 |    24/   91 batches | lr 10.00 | ms/batch 4203.64 | loss  6.71 | ppl   823.11\n",
      "| epoch   3 |    25/   91 batches | lr 10.00 | ms/batch 3999.22 | loss  6.90 | ppl   994.43\n",
      "| epoch   3 |    26/   91 batches | lr 10.00 | ms/batch 4162.66 | loss  7.01 | ppl  1109.32\n",
      "| epoch   3 |    27/   91 batches | lr 10.00 | ms/batch 4037.05 | loss  7.58 | ppl  1964.35\n",
      "| epoch   3 |    28/   91 batches | lr 10.00 | ms/batch 4047.24 | loss  7.54 | ppl  1882.02\n",
      "| epoch   3 |    29/   91 batches | lr 10.00 | ms/batch 3945.39 | loss  7.19 | ppl  1327.88\n",
      "| epoch   3 |    30/   91 batches | lr 10.00 | ms/batch 4106.06 | loss  6.96 | ppl  1057.25\n",
      "| epoch   3 |    31/   91 batches | lr 10.00 | ms/batch 4089.22 | loss  6.97 | ppl  1067.56\n",
      "| epoch   3 |    32/   91 batches | lr 10.00 | ms/batch 4012.51 | loss  6.91 | ppl  1000.60\n",
      "| epoch   3 |    33/   91 batches | lr 10.00 | ms/batch 4055.37 | loss  6.88 | ppl   970.19\n",
      "| epoch   3 |    34/   91 batches | lr 10.00 | ms/batch 4049.36 | loss  6.84 | ppl   935.41\n",
      "| epoch   3 |    35/   91 batches | lr 10.00 | ms/batch 4058.82 | loss  6.91 | ppl   998.99\n",
      "| epoch   3 |    36/   91 batches | lr 10.00 | ms/batch 4068.89 | loss  6.87 | ppl   966.68\n",
      "| epoch   3 |    37/   91 batches | lr 10.00 | ms/batch 4087.75 | loss  6.90 | ppl   992.16\n",
      "| epoch   3 |    38/   91 batches | lr 10.00 | ms/batch 3990.19 | loss  6.89 | ppl   981.43\n",
      "| epoch   3 |    39/   91 batches | lr 10.00 | ms/batch 4085.66 | loss  6.86 | ppl   957.21\n",
      "| epoch   3 |    40/   91 batches | lr 10.00 | ms/batch 4060.84 | loss  6.85 | ppl   940.01\n",
      "| epoch   3 |    41/   91 batches | lr 10.00 | ms/batch 3983.80 | loss  6.81 | ppl   909.96\n",
      "| epoch   3 |    42/   91 batches | lr 10.00 | ms/batch 4014.93 | loss  6.96 | ppl  1054.92\n",
      "| epoch   3 |    43/   91 batches | lr 10.00 | ms/batch 3974.83 | loss  6.81 | ppl   911.05\n",
      "| epoch   3 |    44/   91 batches | lr 10.00 | ms/batch 4081.94 | loss  6.94 | ppl  1029.50\n",
      "| epoch   3 |    45/   91 batches | lr 10.00 | ms/batch 4179.93 | loss  6.94 | ppl  1037.38\n",
      "| epoch   3 |    46/   91 batches | lr 10.00 | ms/batch 3842.92 | loss  6.90 | ppl   994.81\n",
      "| epoch   3 |    47/   91 batches | lr 10.00 | ms/batch 4010.93 | loss  7.01 | ppl  1102.88\n",
      "| epoch   3 |    48/   91 batches | lr 10.00 | ms/batch 4056.23 | loss  6.83 | ppl   923.74\n",
      "| epoch   3 |    49/   91 batches | lr 10.00 | ms/batch 4185.79 | loss  6.89 | ppl   978.64\n",
      "| epoch   3 |    50/   91 batches | lr 10.00 | ms/batch 4092.37 | loss  6.83 | ppl   924.26\n",
      "| epoch   3 |    51/   91 batches | lr 10.00 | ms/batch 4116.69 | loss  6.90 | ppl   995.99\n",
      "| epoch   3 |    52/   91 batches | lr 10.00 | ms/batch 4049.45 | loss  6.91 | ppl   998.11\n",
      "| epoch   3 |    53/   91 batches | lr 10.00 | ms/batch 4039.67 | loss  6.87 | ppl   960.74\n",
      "| epoch   3 |    54/   91 batches | lr 10.00 | ms/batch 3935.34 | loss  6.92 | ppl  1012.46\n",
      "| epoch   3 |    55/   91 batches | lr 10.00 | ms/batch 4067.90 | loss  6.91 | ppl  1001.00\n",
      "| epoch   3 |    56/   91 batches | lr 10.00 | ms/batch 3918.53 | loss  6.93 | ppl  1018.19\n",
      "| epoch   3 |    57/   91 batches | lr 10.00 | ms/batch 3900.70 | loss  6.89 | ppl   985.72\n",
      "| epoch   3 |    58/   91 batches | lr 10.00 | ms/batch 4403.16 | loss  7.12 | ppl  1233.20\n",
      "| epoch   3 |    59/   91 batches | lr 10.00 | ms/batch 4031.24 | loss  6.92 | ppl  1014.82\n",
      "| epoch   3 |    60/   91 batches | lr 10.00 | ms/batch 4049.47 | loss  6.84 | ppl   937.72\n",
      "| epoch   3 |    61/   91 batches | lr 10.00 | ms/batch 4163.62 | loss  6.87 | ppl   958.97\n",
      "| epoch   3 |    62/   91 batches | lr 10.00 | ms/batch 4091.64 | loss  6.82 | ppl   918.24\n",
      "| epoch   3 |    63/   91 batches | lr 10.00 | ms/batch 3974.02 | loss  6.74 | ppl   849.02\n",
      "| epoch   3 |    64/   91 batches | lr 10.00 | ms/batch 4025.13 | loss  6.82 | ppl   918.16\n",
      "| epoch   3 |    65/   91 batches | lr 10.00 | ms/batch 4064.47 | loss  6.96 | ppl  1057.38\n",
      "| epoch   3 |    66/   91 batches | lr 10.00 | ms/batch 4033.73 | loss  6.87 | ppl   958.95\n",
      "| epoch   3 |    67/   91 batches | lr 10.00 | ms/batch 4031.71 | loss  6.80 | ppl   899.08\n",
      "| epoch   3 |    68/   91 batches | lr 10.00 | ms/batch 4032.12 | loss  6.92 | ppl  1012.61\n",
      "| epoch   3 |    69/   91 batches | lr 10.00 | ms/batch 4068.66 | loss  6.82 | ppl   914.56\n",
      "| epoch   3 |    70/   91 batches | lr 10.00 | ms/batch 4050.99 | loss  6.75 | ppl   850.45\n",
      "| epoch   3 |    71/   91 batches | lr 10.00 | ms/batch 4328.33 | loss  6.82 | ppl   913.97\n",
      "| epoch   3 |    72/   91 batches | lr 10.00 | ms/batch 3974.20 | loss  6.83 | ppl   921.54\n",
      "| epoch   3 |    73/   91 batches | lr 10.00 | ms/batch 4066.86 | loss  7.04 | ppl  1142.19\n",
      "| epoch   3 |    74/   91 batches | lr 10.00 | ms/batch 4024.40 | loss  7.01 | ppl  1108.57\n",
      "| epoch   3 |    75/   91 batches | lr 10.00 | ms/batch 4003.81 | loss  6.88 | ppl   968.95\n",
      "| epoch   3 |    76/   91 batches | lr 10.00 | ms/batch 3978.00 | loss  6.67 | ppl   786.64\n",
      "| epoch   3 |    77/   91 batches | lr 10.00 | ms/batch 4020.55 | loss  6.75 | ppl   856.20\n",
      "| epoch   3 |    78/   91 batches | lr 10.00 | ms/batch 4473.76 | loss  6.93 | ppl  1025.81\n",
      "| epoch   3 |    79/   91 batches | lr 10.00 | ms/batch 4131.66 | loss  7.11 | ppl  1224.79\n",
      "| epoch   3 |    80/   91 batches | lr 10.00 | ms/batch 4206.91 | loss  7.00 | ppl  1093.67\n",
      "| epoch   3 |    81/   91 batches | lr 10.00 | ms/batch 4109.00 | loss  6.92 | ppl  1012.48\n",
      "| epoch   3 |    82/   91 batches | lr 10.00 | ms/batch 4032.34 | loss  6.93 | ppl  1023.75\n",
      "| epoch   3 |    83/   91 batches | lr 10.00 | ms/batch 3995.33 | loss  6.91 | ppl  1007.12\n",
      "| epoch   3 |    84/   91 batches | lr 10.00 | ms/batch 4054.59 | loss  6.81 | ppl   909.48\n",
      "| epoch   3 |    85/   91 batches | lr 10.00 | ms/batch 4036.87 | loss  6.68 | ppl   796.20\n",
      "| epoch   3 |    86/   91 batches | lr 10.00 | ms/batch 4233.20 | loss  6.70 | ppl   810.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |    87/   91 batches | lr 10.00 | ms/batch 3984.40 | loss  6.78 | ppl   877.21\n",
      "| epoch   3 |    88/   91 batches | lr 10.00 | ms/batch 4029.96 | loss  7.01 | ppl  1109.28\n",
      "| epoch   3 |    89/   91 batches | lr 10.00 | ms/batch 3952.29 | loss  6.96 | ppl  1049.47\n",
      "| epoch   3 |    90/   91 batches | lr 10.00 | ms/batch 4068.57 | loss  6.71 | ppl   818.67\n",
      "| epoch   3 |    91/   91 batches | lr 10.00 | ms/batch 2964.52 | loss  6.78 | ppl   875.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 405.53s | valid loss  0.22 | valid ppl     1.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |     1/   91 batches | lr 10.00 | ms/batch 7929.33 | loss 13.53 | ppl 748099.27\n",
      "| epoch   4 |     2/   91 batches | lr 10.00 | ms/batch 4182.62 | loss  6.71 | ppl   816.90\n",
      "| epoch   4 |     3/   91 batches | lr 10.00 | ms/batch 4027.14 | loss  6.82 | ppl   919.39\n",
      "| epoch   4 |     4/   91 batches | lr 10.00 | ms/batch 3982.05 | loss  6.81 | ppl   903.16\n",
      "| epoch   4 |     5/   91 batches | lr 10.00 | ms/batch 3957.35 | loss  6.70 | ppl   811.77\n",
      "| epoch   4 |     6/   91 batches | lr 10.00 | ms/batch 3945.68 | loss  6.72 | ppl   829.48\n",
      "| epoch   4 |     7/   91 batches | lr 10.00 | ms/batch 3989.39 | loss  6.75 | ppl   856.93\n",
      "| epoch   4 |     8/   91 batches | lr 10.00 | ms/batch 3987.10 | loss  6.90 | ppl   996.51\n",
      "| epoch   4 |     9/   91 batches | lr 10.00 | ms/batch 3995.17 | loss  6.92 | ppl  1015.32\n",
      "| epoch   4 |    10/   91 batches | lr 10.00 | ms/batch 4119.03 | loss  6.72 | ppl   824.69\n",
      "| epoch   4 |    11/   91 batches | lr 10.00 | ms/batch 4030.96 | loss  6.81 | ppl   908.02\n",
      "| epoch   4 |    12/   91 batches | lr 10.00 | ms/batch 4000.36 | loss  6.86 | ppl   953.88\n",
      "| epoch   4 |    13/   91 batches | lr 10.00 | ms/batch 3974.62 | loss  6.74 | ppl   848.82\n",
      "| epoch   4 |    14/   91 batches | lr 10.00 | ms/batch 3981.15 | loss  6.83 | ppl   921.37\n",
      "| epoch   4 |    15/   91 batches | lr 10.00 | ms/batch 3981.20 | loss  6.76 | ppl   859.94\n",
      "| epoch   4 |    16/   91 batches | lr 10.00 | ms/batch 4063.01 | loss  6.84 | ppl   932.63\n",
      "| epoch   4 |    17/   91 batches | lr 10.00 | ms/batch 3940.95 | loss  6.87 | ppl   967.14\n",
      "| epoch   4 |    18/   91 batches | lr 10.00 | ms/batch 3946.23 | loss  6.82 | ppl   918.86\n",
      "| epoch   4 |    19/   91 batches | lr 10.00 | ms/batch 3956.96 | loss  6.80 | ppl   894.61\n",
      "| epoch   4 |    20/   91 batches | lr 10.00 | ms/batch 3897.09 | loss  6.78 | ppl   876.01\n",
      "| epoch   4 |    21/   91 batches | lr 10.00 | ms/batch 3978.71 | loss  6.65 | ppl   775.42\n",
      "| epoch   4 |    22/   91 batches | lr 10.00 | ms/batch 4070.71 | loss  6.80 | ppl   898.24\n",
      "| epoch   4 |    23/   91 batches | lr 10.00 | ms/batch 3983.52 | loss  6.83 | ppl   926.16\n",
      "| epoch   4 |    24/   91 batches | lr 10.00 | ms/batch 3977.62 | loss  6.82 | ppl   915.06\n",
      "| epoch   4 |    25/   91 batches | lr 10.00 | ms/batch 3910.57 | loss  6.77 | ppl   874.84\n",
      "| epoch   4 |    26/   91 batches | lr 10.00 | ms/batch 3998.93 | loss  6.63 | ppl   758.80\n",
      "| epoch   4 |    27/   91 batches | lr 10.00 | ms/batch 3974.43 | loss  6.82 | ppl   912.95\n",
      "| epoch   4 |    28/   91 batches | lr 10.00 | ms/batch 4024.99 | loss  6.74 | ppl   849.67\n",
      "| epoch   4 |    29/   91 batches | lr 10.00 | ms/batch 3997.45 | loss  6.81 | ppl   905.09\n",
      "| epoch   4 |    30/   91 batches | lr 10.00 | ms/batch 3825.62 | loss  6.64 | ppl   766.16\n",
      "| epoch   4 |    31/   91 batches | lr 10.00 | ms/batch 4009.98 | loss  6.85 | ppl   942.63\n",
      "| epoch   4 |    32/   91 batches | lr 10.00 | ms/batch 3796.87 | loss  6.85 | ppl   942.69\n",
      "| epoch   4 |    33/   91 batches | lr 10.00 | ms/batch 3991.85 | loss  6.72 | ppl   827.30\n",
      "| epoch   4 |    34/   91 batches | lr 10.00 | ms/batch 4009.82 | loss  6.67 | ppl   791.37\n",
      "| epoch   4 |    35/   91 batches | lr 10.00 | ms/batch 3985.85 | loss  6.78 | ppl   880.17\n",
      "| epoch   4 |    36/   91 batches | lr 10.00 | ms/batch 3985.23 | loss  6.70 | ppl   815.41\n",
      "| epoch   4 |    37/   91 batches | lr 10.00 | ms/batch 3950.38 | loss  6.72 | ppl   831.83\n",
      "| epoch   4 |    38/   91 batches | lr 10.00 | ms/batch 4004.74 | loss  6.73 | ppl   838.61\n",
      "| epoch   4 |    39/   91 batches | lr 10.00 | ms/batch 4190.66 | loss  6.78 | ppl   876.05\n",
      "| epoch   4 |    40/   91 batches | lr 10.00 | ms/batch 4077.16 | loss  6.66 | ppl   781.50\n",
      "| epoch   4 |    41/   91 batches | lr 10.00 | ms/batch 4005.53 | loss  6.62 | ppl   752.40\n",
      "| epoch   4 |    42/   91 batches | lr 10.00 | ms/batch 3980.53 | loss  6.70 | ppl   815.68\n",
      "| epoch   4 |    43/   91 batches | lr 10.00 | ms/batch 4703.50 | loss  6.64 | ppl   766.58\n",
      "| epoch   4 |    44/   91 batches | lr 10.00 | ms/batch 4764.81 | loss  6.78 | ppl   877.74\n",
      "| epoch   4 |    45/   91 batches | lr 10.00 | ms/batch 4354.93 | loss  6.74 | ppl   845.23\n",
      "| epoch   4 |    46/   91 batches | lr 10.00 | ms/batch 4035.58 | loss  6.66 | ppl   782.36\n",
      "| epoch   4 |    47/   91 batches | lr 10.00 | ms/batch 4060.34 | loss  6.85 | ppl   944.66\n",
      "| epoch   4 |    48/   91 batches | lr 10.00 | ms/batch 4049.17 | loss  6.64 | ppl   761.92\n",
      "| epoch   4 |    49/   91 batches | lr 10.00 | ms/batch 4037.25 | loss  6.74 | ppl   844.97\n",
      "| epoch   4 |    50/   91 batches | lr 10.00 | ms/batch 3973.48 | loss  6.67 | ppl   788.49\n",
      "| epoch   4 |    51/   91 batches | lr 10.00 | ms/batch 3991.61 | loss  6.87 | ppl   959.66\n",
      "| epoch   4 |    52/   91 batches | lr 10.00 | ms/batch 4056.98 | loss  6.76 | ppl   861.05\n",
      "| epoch   4 |    53/   91 batches | lr 10.00 | ms/batch 3970.95 | loss  6.66 | ppl   784.07\n",
      "| epoch   4 |    54/   91 batches | lr 10.00 | ms/batch 4000.00 | loss  6.72 | ppl   829.50\n",
      "| epoch   4 |    55/   91 batches | lr 10.00 | ms/batch 4145.77 | loss  6.80 | ppl   901.96\n",
      "| epoch   4 |    56/   91 batches | lr 10.00 | ms/batch 4073.15 | loss  6.89 | ppl   978.26\n",
      "| epoch   4 |    57/   91 batches | lr 10.00 | ms/batch 4036.65 | loss  6.72 | ppl   828.80\n",
      "| epoch   4 |    58/   91 batches | lr 10.00 | ms/batch 3908.27 | loss  6.72 | ppl   831.26\n",
      "| epoch   4 |    59/   91 batches | lr 10.00 | ms/batch 3974.54 | loss  6.57 | ppl   715.31\n",
      "| epoch   4 |    60/   91 batches | lr 10.00 | ms/batch 4031.16 | loss  6.61 | ppl   739.43\n",
      "| epoch   4 |    61/   91 batches | lr 10.00 | ms/batch 3992.65 | loss  6.82 | ppl   912.54\n",
      "| epoch   4 |    62/   91 batches | lr 10.00 | ms/batch 4127.70 | loss  6.61 | ppl   745.20\n",
      "| epoch   4 |    63/   91 batches | lr 10.00 | ms/batch 3960.99 | loss  6.66 | ppl   783.32\n",
      "| epoch   4 |    64/   91 batches | lr 10.00 | ms/batch 3863.30 | loss  6.67 | ppl   791.00\n",
      "| epoch   4 |    65/   91 batches | lr 10.00 | ms/batch 3975.74 | loss  6.72 | ppl   826.91\n",
      "| epoch   4 |    66/   91 batches | lr 10.00 | ms/batch 3954.51 | loss  6.71 | ppl   821.94\n",
      "| epoch   4 |    67/   91 batches | lr 10.00 | ms/batch 3783.09 | loss  6.69 | ppl   802.00\n",
      "| epoch   4 |    68/   91 batches | lr 10.00 | ms/batch 3942.52 | loss  6.65 | ppl   770.95\n",
      "| epoch   4 |    69/   91 batches | lr 10.00 | ms/batch 4001.39 | loss  6.63 | ppl   760.94\n",
      "| epoch   4 |    70/   91 batches | lr 10.00 | ms/batch 4055.92 | loss  6.59 | ppl   724.70\n",
      "| epoch   4 |    71/   91 batches | lr 10.00 | ms/batch 3875.25 | loss  6.63 | ppl   754.16\n",
      "| epoch   4 |    72/   91 batches | lr 10.00 | ms/batch 3981.73 | loss  6.79 | ppl   890.41\n",
      "| epoch   4 |    73/   91 batches | lr 10.00 | ms/batch 3909.54 | loss  6.70 | ppl   812.89\n",
      "| epoch   4 |    74/   91 batches | lr 10.00 | ms/batch 4024.21 | loss  6.65 | ppl   771.05\n",
      "| epoch   4 |    75/   91 batches | lr 10.00 | ms/batch 3938.31 | loss  6.75 | ppl   854.93\n",
      "| epoch   4 |    76/   91 batches | lr 10.00 | ms/batch 3951.57 | loss  6.52 | ppl   675.50\n",
      "| epoch   4 |    77/   91 batches | lr 10.00 | ms/batch 3783.85 | loss  6.53 | ppl   686.32\n",
      "| epoch   4 |    78/   91 batches | lr 10.00 | ms/batch 3992.59 | loss  6.76 | ppl   859.12\n",
      "| epoch   4 |    79/   91 batches | lr 10.00 | ms/batch 3854.35 | loss  6.74 | ppl   847.55\n",
      "| epoch   4 |    80/   91 batches | lr 10.00 | ms/batch 4060.98 | loss  6.72 | ppl   825.00\n",
      "| epoch   4 |    81/   91 batches | lr 10.00 | ms/batch 3957.75 | loss  6.78 | ppl   876.10\n",
      "| epoch   4 |    82/   91 batches | lr 10.00 | ms/batch 3919.32 | loss  6.69 | ppl   805.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |    83/   91 batches | lr 10.00 | ms/batch 3953.52 | loss  6.72 | ppl   826.84\n",
      "| epoch   4 |    84/   91 batches | lr 10.00 | ms/batch 3991.38 | loss  6.64 | ppl   766.56\n",
      "| epoch   4 |    85/   91 batches | lr 10.00 | ms/batch 3986.56 | loss  6.56 | ppl   709.56\n",
      "| epoch   4 |    86/   91 batches | lr 10.00 | ms/batch 3786.90 | loss  6.56 | ppl   709.70\n",
      "| epoch   4 |    87/   91 batches | lr 10.00 | ms/batch 3996.61 | loss  6.74 | ppl   845.04\n",
      "| epoch   4 |    88/   91 batches | lr 10.00 | ms/batch 3840.35 | loss  6.65 | ppl   773.84\n",
      "| epoch   4 |    89/   91 batches | lr 10.00 | ms/batch 3929.83 | loss  6.65 | ppl   771.43\n",
      "| epoch   4 |    90/   91 batches | lr 10.00 | ms/batch 3934.74 | loss  6.62 | ppl   748.74\n",
      "| epoch   4 |    91/   91 batches | lr 10.00 | ms/batch 2870.32 | loss  6.63 | ppl   760.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 393.64s | valid loss  0.22 | valid ppl     1.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |     1/   91 batches | lr 10.00 | ms/batch 7929.73 | loss 13.30 | ppl 599312.14\n",
      "| epoch   5 |     2/   91 batches | lr 10.00 | ms/batch 3956.83 | loss  6.60 | ppl   738.46\n",
      "| epoch   5 |     3/   91 batches | lr 10.00 | ms/batch 4040.95 | loss  6.41 | ppl   610.51\n",
      "| epoch   5 |     4/   91 batches | lr 10.00 | ms/batch 3932.49 | loss  6.57 | ppl   711.41\n",
      "| epoch   5 |     5/   91 batches | lr 10.00 | ms/batch 3810.66 | loss  6.56 | ppl   706.85\n",
      "| epoch   5 |     6/   91 batches | lr 10.00 | ms/batch 3902.50 | loss  6.61 | ppl   743.84\n",
      "| epoch   5 |     7/   91 batches | lr 10.00 | ms/batch 3965.31 | loss  6.61 | ppl   743.57\n",
      "| epoch   5 |     8/   91 batches | lr 10.00 | ms/batch 3925.64 | loss  6.62 | ppl   746.33\n",
      "| epoch   5 |     9/   91 batches | lr 10.00 | ms/batch 3940.83 | loss  6.65 | ppl   776.05\n",
      "| epoch   5 |    10/   91 batches | lr 10.00 | ms/batch 4009.83 | loss  6.57 | ppl   711.36\n",
      "| epoch   5 |    11/   91 batches | lr 10.00 | ms/batch 3881.25 | loss  6.70 | ppl   814.26\n",
      "| epoch   5 |    12/   91 batches | lr 10.00 | ms/batch 3813.93 | loss  6.71 | ppl   822.90\n",
      "| epoch   5 |    13/   91 batches | lr 10.00 | ms/batch 4040.15 | loss  6.54 | ppl   691.73\n",
      "| epoch   5 |    14/   91 batches | lr 10.00 | ms/batch 3817.72 | loss  6.64 | ppl   766.40\n",
      "| epoch   5 |    15/   91 batches | lr 10.00 | ms/batch 3744.94 | loss  6.67 | ppl   789.81\n",
      "| epoch   5 |    16/   91 batches | lr 10.00 | ms/batch 4128.77 | loss  6.60 | ppl   732.76\n",
      "| epoch   5 |    17/   91 batches | lr 10.00 | ms/batch 3964.04 | loss  6.57 | ppl   711.29\n",
      "| epoch   5 |    18/   91 batches | lr 10.00 | ms/batch 3979.99 | loss  6.65 | ppl   769.84\n",
      "| epoch   5 |    19/   91 batches | lr 10.00 | ms/batch 3987.65 | loss  6.57 | ppl   714.84\n",
      "| epoch   5 |    20/   91 batches | lr 10.00 | ms/batch 3945.25 | loss  6.62 | ppl   751.79\n",
      "| epoch   5 |    21/   91 batches | lr 10.00 | ms/batch 4028.75 | loss  6.59 | ppl   730.22\n",
      "| epoch   5 |    22/   91 batches | lr 10.00 | ms/batch 3951.73 | loss  6.56 | ppl   706.19\n",
      "| epoch   5 |    23/   91 batches | lr 10.00 | ms/batch 3979.49 | loss  6.65 | ppl   769.79\n",
      "| epoch   5 |    24/   91 batches | lr 10.00 | ms/batch 3934.37 | loss  6.49 | ppl   656.17\n",
      "| epoch   5 |    25/   91 batches | lr 10.00 | ms/batch 3983.14 | loss  6.56 | ppl   705.70\n",
      "| epoch   5 |    26/   91 batches | lr 10.00 | ms/batch 3880.82 | loss  6.52 | ppl   680.26\n",
      "| epoch   5 |    27/   91 batches | lr 10.00 | ms/batch 3922.18 | loss  6.87 | ppl   963.55\n",
      "| epoch   5 |    28/   91 batches | lr 10.00 | ms/batch 3902.75 | loss  6.59 | ppl   731.39\n",
      "| epoch   5 |    29/   91 batches | lr 10.00 | ms/batch 3953.13 | loss  6.54 | ppl   693.21\n",
      "| epoch   5 |    30/   91 batches | lr 10.00 | ms/batch 3896.11 | loss  6.55 | ppl   700.85\n",
      "| epoch   5 |    31/   91 batches | lr 10.00 | ms/batch 3892.79 | loss  6.67 | ppl   789.42\n",
      "| epoch   5 |    32/   91 batches | lr 10.00 | ms/batch 4024.07 | loss  6.59 | ppl   728.42\n",
      "| epoch   5 |    33/   91 batches | lr 10.00 | ms/batch 4008.64 | loss  6.65 | ppl   774.54\n",
      "| epoch   5 |    34/   91 batches | lr 10.00 | ms/batch 3980.39 | loss  6.66 | ppl   784.32\n",
      "| epoch   5 |    35/   91 batches | lr 10.00 | ms/batch 4020.15 | loss  6.54 | ppl   690.00\n",
      "| epoch   5 |    36/   91 batches | lr 10.00 | ms/batch 3892.60 | loss  6.54 | ppl   691.26\n",
      "| epoch   5 |    37/   91 batches | lr 10.00 | ms/batch 3927.45 | loss  6.56 | ppl   706.77\n",
      "| epoch   5 |    38/   91 batches | lr 10.00 | ms/batch 3935.42 | loss  6.49 | ppl   657.55\n",
      "| epoch   5 |    39/   91 batches | lr 10.00 | ms/batch 4015.76 | loss  6.71 | ppl   820.58\n",
      "| epoch   5 |    40/   91 batches | lr 10.00 | ms/batch 3948.49 | loss  6.58 | ppl   722.08\n",
      "| epoch   5 |    41/   91 batches | lr 10.00 | ms/batch 3913.65 | loss  6.56 | ppl   707.06\n",
      "| epoch   5 |    42/   91 batches | lr 10.00 | ms/batch 4060.43 | loss  6.51 | ppl   672.01\n",
      "| epoch   5 |    43/   91 batches | lr 10.00 | ms/batch 3875.94 | loss  6.40 | ppl   601.75\n",
      "| epoch   5 |    44/   91 batches | lr 10.00 | ms/batch 3902.19 | loss  6.44 | ppl   627.61\n",
      "| epoch   5 |    45/   91 batches | lr 10.00 | ms/batch 4003.89 | loss  6.62 | ppl   748.45\n",
      "| epoch   5 |    46/   91 batches | lr 10.00 | ms/batch 3867.84 | loss  6.75 | ppl   852.09\n",
      "| epoch   5 |    47/   91 batches | lr 10.00 | ms/batch 3835.10 | loss  6.78 | ppl   877.01\n",
      "| epoch   5 |    48/   91 batches | lr 10.00 | ms/batch 4005.72 | loss  6.46 | ppl   641.94\n",
      "| epoch   5 |    49/   91 batches | lr 10.00 | ms/batch 3824.10 | loss  6.56 | ppl   706.20\n",
      "| epoch   5 |    50/   91 batches | lr 10.00 | ms/batch 3943.20 | loss  6.51 | ppl   671.64\n",
      "| epoch   5 |    51/   91 batches | lr 10.00 | ms/batch 3996.67 | loss  6.60 | ppl   734.56\n",
      "| epoch   5 |    52/   91 batches | lr 10.00 | ms/batch 3928.57 | loss  6.54 | ppl   689.82\n",
      "| epoch   5 |    53/   91 batches | lr 10.00 | ms/batch 3853.67 | loss  6.50 | ppl   665.91\n",
      "| epoch   5 |    54/   91 batches | lr 10.00 | ms/batch 3853.28 | loss  6.57 | ppl   711.02\n",
      "| epoch   5 |    55/   91 batches | lr 10.00 | ms/batch 3962.30 | loss  6.63 | ppl   758.84\n",
      "| epoch   5 |    56/   91 batches | lr 10.00 | ms/batch 4009.26 | loss  6.87 | ppl   960.79\n",
      "| epoch   5 |    57/   91 batches | lr 10.00 | ms/batch 4001.16 | loss  6.51 | ppl   672.94\n",
      "| epoch   5 |    58/   91 batches | lr 10.00 | ms/batch 3941.73 | loss  6.57 | ppl   710.03\n",
      "| epoch   5 |    59/   91 batches | lr 10.00 | ms/batch 3985.51 | loss  6.42 | ppl   612.49\n",
      "| epoch   5 |    60/   91 batches | lr 10.00 | ms/batch 3962.43 | loss  6.50 | ppl   666.53\n",
      "| epoch   5 |    61/   91 batches | lr 10.00 | ms/batch 4007.99 | loss  6.71 | ppl   822.36\n",
      "| epoch   5 |    62/   91 batches | lr 10.00 | ms/batch 3935.20 | loss  6.58 | ppl   722.70\n",
      "| epoch   5 |    63/   91 batches | lr 10.00 | ms/batch 4014.68 | loss  6.44 | ppl   626.08\n",
      "| epoch   5 |    64/   91 batches | lr 10.00 | ms/batch 3949.44 | loss  6.49 | ppl   661.38\n",
      "| epoch   5 |    65/   91 batches | lr 10.00 | ms/batch 3910.67 | loss  6.58 | ppl   720.55\n",
      "| epoch   5 |    66/   91 batches | lr 10.00 | ms/batch 3937.32 | loss  6.54 | ppl   695.08\n",
      "| epoch   5 |    67/   91 batches | lr 10.00 | ms/batch 3982.00 | loss  6.45 | ppl   630.62\n",
      "| epoch   5 |    68/   91 batches | lr 10.00 | ms/batch 3829.11 | loss  6.48 | ppl   651.03\n",
      "| epoch   5 |    69/   91 batches | lr 10.00 | ms/batch 3972.76 | loss  6.59 | ppl   726.50\n",
      "| epoch   5 |    70/   91 batches | lr 10.00 | ms/batch 3917.31 | loss  6.54 | ppl   694.56\n",
      "| epoch   5 |    71/   91 batches | lr 10.00 | ms/batch 3898.31 | loss  6.42 | ppl   612.66\n",
      "| epoch   5 |    72/   91 batches | lr 10.00 | ms/batch 3935.37 | loss  6.53 | ppl   688.37\n",
      "| epoch   5 |    73/   91 batches | lr 10.00 | ms/batch 3953.12 | loss  6.65 | ppl   770.86\n",
      "| epoch   5 |    74/   91 batches | lr 10.00 | ms/batch 3994.67 | loss  6.53 | ppl   684.69\n",
      "| epoch   5 |    75/   91 batches | lr 10.00 | ms/batch 4018.78 | loss  6.64 | ppl   762.14\n",
      "| epoch   5 |    76/   91 batches | lr 10.00 | ms/batch 3936.53 | loss  6.49 | ppl   657.23\n",
      "| epoch   5 |    77/   91 batches | lr 10.00 | ms/batch 3935.60 | loss  6.46 | ppl   637.55\n",
      "| epoch   5 |    78/   91 batches | lr 10.00 | ms/batch 3978.43 | loss  6.55 | ppl   702.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |    79/   91 batches | lr 10.00 | ms/batch 3937.48 | loss  6.51 | ppl   672.81\n",
      "| epoch   5 |    80/   91 batches | lr 10.00 | ms/batch 3872.39 | loss  6.54 | ppl   694.07\n",
      "| epoch   5 |    81/   91 batches | lr 10.00 | ms/batch 3946.20 | loss  6.67 | ppl   788.86\n",
      "| epoch   5 |    82/   91 batches | lr 10.00 | ms/batch 3831.92 | loss  6.54 | ppl   695.50\n",
      "| epoch   5 |    83/   91 batches | lr 10.00 | ms/batch 3803.60 | loss  6.58 | ppl   720.35\n",
      "| epoch   5 |    84/   91 batches | lr 10.00 | ms/batch 3913.68 | loss  6.55 | ppl   700.13\n",
      "| epoch   5 |    85/   91 batches | lr 10.00 | ms/batch 3869.53 | loss  6.55 | ppl   697.96\n",
      "| epoch   5 |    86/   91 batches | lr 10.00 | ms/batch 3825.15 | loss  6.42 | ppl   610.97\n",
      "| epoch   5 |    87/   91 batches | lr 10.00 | ms/batch 4130.08 | loss  6.43 | ppl   620.18\n",
      "| epoch   5 |    88/   91 batches | lr 10.00 | ms/batch 3968.44 | loss  6.59 | ppl   727.23\n",
      "| epoch   5 |    89/   91 batches | lr 10.00 | ms/batch 3981.83 | loss  6.50 | ppl   668.36\n",
      "| epoch   5 |    90/   91 batches | lr 10.00 | ms/batch 3990.34 | loss  6.43 | ppl   620.98\n",
      "| epoch   5 |    91/   91 batches | lr 10.00 | ms/batch 2934.30 | loss  6.48 | ppl   651.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 388.82s | valid loss  0.22 | valid ppl     1.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |     1/   91 batches | lr 10.00 | ms/batch 7900.97 | loss 13.16 | ppl 517133.29\n",
      "| epoch   6 |     2/   91 batches | lr 10.00 | ms/batch 3976.44 | loss  6.46 | ppl   637.90\n",
      "| epoch   6 |     3/   91 batches | lr 10.00 | ms/batch 3897.15 | loss  6.55 | ppl   698.12\n",
      "| epoch   6 |     4/   91 batches | lr 10.00 | ms/batch 3813.96 | loss  6.54 | ppl   693.08\n",
      "| epoch   6 |     5/   91 batches | lr 10.00 | ms/batch 4041.15 | loss  6.54 | ppl   692.00\n",
      "| epoch   6 |     6/   91 batches | lr 10.00 | ms/batch 3942.78 | loss  6.62 | ppl   749.49\n",
      "| epoch   6 |     7/   91 batches | lr 10.00 | ms/batch 3937.74 | loss  6.52 | ppl   677.57\n",
      "| epoch   6 |     8/   91 batches | lr 10.00 | ms/batch 3964.45 | loss  6.43 | ppl   621.64\n",
      "| epoch   6 |     9/   91 batches | lr 10.00 | ms/batch 3961.12 | loss  6.52 | ppl   681.55\n",
      "| epoch   6 |    10/   91 batches | lr 10.00 | ms/batch 4223.25 | loss  6.38 | ppl   591.31\n",
      "| epoch   6 |    11/   91 batches | lr 10.00 | ms/batch 4338.59 | loss  6.44 | ppl   623.70\n",
      "| epoch   6 |    12/   91 batches | lr 10.00 | ms/batch 4280.50 | loss  6.46 | ppl   640.35\n",
      "| epoch   6 |    13/   91 batches | lr 10.00 | ms/batch 4017.17 | loss  6.51 | ppl   669.44\n",
      "| epoch   6 |    14/   91 batches | lr 10.00 | ms/batch 4001.43 | loss  6.56 | ppl   707.73\n",
      "| epoch   6 |    15/   91 batches | lr 10.00 | ms/batch 3954.67 | loss  6.61 | ppl   740.47\n",
      "| epoch   6 |    16/   91 batches | lr 10.00 | ms/batch 3901.18 | loss  6.46 | ppl   637.53\n",
      "| epoch   6 |    17/   91 batches | lr 10.00 | ms/batch 3831.16 | loss  6.47 | ppl   647.73\n",
      "| epoch   6 |    18/   91 batches | lr 10.00 | ms/batch 3868.59 | loss  6.45 | ppl   632.44\n",
      "| epoch   6 |    19/   91 batches | lr 10.00 | ms/batch 3976.62 | loss  6.40 | ppl   599.47\n",
      "| epoch   6 |    20/   91 batches | lr 10.00 | ms/batch 3855.54 | loss  6.54 | ppl   691.39\n",
      "| epoch   6 |    21/   91 batches | lr 10.00 | ms/batch 3919.22 | loss  6.43 | ppl   619.91\n",
      "| epoch   6 |    22/   91 batches | lr 10.00 | ms/batch 3797.55 | loss  6.49 | ppl   656.97\n",
      "| epoch   6 |    23/   91 batches | lr 10.00 | ms/batch 4025.04 | loss  6.67 | ppl   788.39\n",
      "| epoch   6 |    24/   91 batches | lr 10.00 | ms/batch 3958.26 | loss  6.30 | ppl   545.19\n",
      "| epoch   6 |    25/   91 batches | lr 10.00 | ms/batch 4030.00 | loss  6.45 | ppl   634.51\n",
      "| epoch   6 |    26/   91 batches | lr 10.00 | ms/batch 3982.70 | loss  6.51 | ppl   670.61\n",
      "| epoch   6 |    27/   91 batches | lr 10.00 | ms/batch 3894.37 | loss  6.59 | ppl   727.13\n",
      "| epoch   6 |    28/   91 batches | lr 10.00 | ms/batch 4024.04 | loss  6.49 | ppl   660.27\n",
      "| epoch   6 |    29/   91 batches | lr 10.00 | ms/batch 3932.08 | loss  6.48 | ppl   655.00\n",
      "| epoch   6 |    30/   91 batches | lr 10.00 | ms/batch 3791.24 | loss  6.39 | ppl   595.71\n",
      "| epoch   6 |    31/   91 batches | lr 10.00 | ms/batch 3842.72 | loss  6.55 | ppl   695.77\n",
      "| epoch   6 |    32/   91 batches | lr 10.00 | ms/batch 3905.25 | loss  6.56 | ppl   707.34\n",
      "| epoch   6 |    33/   91 batches | lr 10.00 | ms/batch 3804.97 | loss  6.45 | ppl   635.13\n",
      "| epoch   6 |    34/   91 batches | lr 10.00 | ms/batch 3865.67 | loss  6.49 | ppl   656.48\n",
      "| epoch   6 |    35/   91 batches | lr 10.00 | ms/batch 3966.63 | loss  6.35 | ppl   571.69\n",
      "| epoch   6 |    36/   91 batches | lr 10.00 | ms/batch 4002.53 | loss  6.45 | ppl   630.13\n",
      "| epoch   6 |    37/   91 batches | lr 10.00 | ms/batch 3849.44 | loss  6.47 | ppl   642.90\n",
      "| epoch   6 |    38/   91 batches | lr 10.00 | ms/batch 3891.43 | loss  6.36 | ppl   576.43\n",
      "| epoch   6 |    39/   91 batches | lr 10.00 | ms/batch 4474.69 | loss  6.57 | ppl   711.81\n",
      "| epoch   6 |    40/   91 batches | lr 10.00 | ms/batch 10613.65 | loss  6.42 | ppl   616.07\n",
      "| epoch   6 |    41/   91 batches | lr 10.00 | ms/batch 6736.62 | loss  6.51 | ppl   673.03\n",
      "| epoch   6 |    42/   91 batches | lr 10.00 | ms/batch 5307.74 | loss  6.48 | ppl   652.85\n",
      "| epoch   6 |    43/   91 batches | lr 10.00 | ms/batch 4968.81 | loss  6.33 | ppl   559.35\n",
      "| epoch   6 |    44/   91 batches | lr 10.00 | ms/batch 4360.60 | loss  6.43 | ppl   619.25\n",
      "| epoch   6 |    45/   91 batches | lr 10.00 | ms/batch 4245.95 | loss  6.42 | ppl   614.76\n",
      "| epoch   6 |    46/   91 batches | lr 10.00 | ms/batch 4669.71 | loss  6.34 | ppl   566.66\n",
      "| epoch   6 |    47/   91 batches | lr 10.00 | ms/batch 5138.10 | loss  6.43 | ppl   619.02\n",
      "| epoch   6 |    48/   91 batches | lr 10.00 | ms/batch 4044.24 | loss  6.31 | ppl   551.24\n",
      "| epoch   6 |    49/   91 batches | lr 10.00 | ms/batch 4098.34 | loss  6.47 | ppl   647.11\n",
      "| epoch   6 |    50/   91 batches | lr 10.00 | ms/batch 4012.81 | loss  6.44 | ppl   624.68\n",
      "| epoch   6 |    51/   91 batches | lr 10.00 | ms/batch 4054.35 | loss  6.50 | ppl   663.36\n",
      "| epoch   6 |    52/   91 batches | lr 10.00 | ms/batch 3952.93 | loss  6.35 | ppl   574.82\n",
      "| epoch   6 |    53/   91 batches | lr 10.00 | ms/batch 4297.92 | loss  6.51 | ppl   672.87\n",
      "| epoch   6 |    54/   91 batches | lr 10.00 | ms/batch 4442.83 | loss  6.43 | ppl   618.90\n",
      "| epoch   6 |    55/   91 batches | lr 10.00 | ms/batch 4355.83 | loss  6.46 | ppl   637.87\n",
      "| epoch   6 |    56/   91 batches | lr 10.00 | ms/batch 3852.60 | loss  6.54 | ppl   692.40\n",
      "| epoch   6 |    57/   91 batches | lr 10.00 | ms/batch 3919.60 | loss  6.54 | ppl   691.50\n",
      "| epoch   6 |    58/   91 batches | lr 10.00 | ms/batch 3934.43 | loss  6.52 | ppl   677.29\n",
      "| epoch   6 |    59/   91 batches | lr 10.00 | ms/batch 3849.80 | loss  6.39 | ppl   598.10\n",
      "| epoch   6 |    60/   91 batches | lr 10.00 | ms/batch 3909.41 | loss  6.35 | ppl   574.79\n",
      "| epoch   6 |    61/   91 batches | lr 10.00 | ms/batch 3987.01 | loss  6.49 | ppl   657.06\n",
      "| epoch   6 |    62/   91 batches | lr 10.00 | ms/batch 3940.76 | loss  6.40 | ppl   604.34\n",
      "| epoch   6 |    63/   91 batches | lr 10.00 | ms/batch 3802.41 | loss  6.39 | ppl   598.14\n",
      "| epoch   6 |    64/   91 batches | lr 10.00 | ms/batch 4045.38 | loss  6.40 | ppl   604.23\n",
      "| epoch   6 |    65/   91 batches | lr 10.00 | ms/batch 3917.53 | loss  6.42 | ppl   614.11\n",
      "| epoch   6 |    66/   91 batches | lr 10.00 | ms/batch 3988.42 | loss  6.38 | ppl   589.97\n",
      "| epoch   6 |    67/   91 batches | lr 10.00 | ms/batch 3907.45 | loss  6.33 | ppl   562.60\n",
      "| epoch   6 |    68/   91 batches | lr 10.00 | ms/batch 3948.88 | loss  6.39 | ppl   596.42\n",
      "| epoch   6 |    69/   91 batches | lr 10.00 | ms/batch 3919.39 | loss  6.42 | ppl   612.52\n",
      "| epoch   6 |    70/   91 batches | lr 10.00 | ms/batch 3964.97 | loss  6.46 | ppl   638.03\n",
      "| epoch   6 |    71/   91 batches | lr 10.00 | ms/batch 3931.11 | loss  6.30 | ppl   543.89\n",
      "| epoch   6 |    72/   91 batches | lr 10.00 | ms/batch 3845.70 | loss  6.45 | ppl   633.84\n",
      "| epoch   6 |    73/   91 batches | lr 10.00 | ms/batch 4024.58 | loss  6.55 | ppl   697.40\n",
      "| epoch   6 |    74/   91 batches | lr 10.00 | ms/batch 3947.40 | loss  6.65 | ppl   774.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |    75/   91 batches | lr 10.00 | ms/batch 3918.34 | loss  6.47 | ppl   644.42\n",
      "| epoch   6 |    76/   91 batches | lr 10.00 | ms/batch 3932.43 | loss  6.24 | ppl   514.58\n",
      "| epoch   6 |    77/   91 batches | lr 10.00 | ms/batch 3996.77 | loss  6.25 | ppl   516.93\n",
      "| epoch   6 |    78/   91 batches | lr 10.00 | ms/batch 3944.77 | loss  6.44 | ppl   625.69\n",
      "| epoch   6 |    79/   91 batches | lr 10.00 | ms/batch 3874.09 | loss  6.54 | ppl   690.91\n",
      "| epoch   6 |    80/   91 batches | lr 10.00 | ms/batch 4120.47 | loss  6.47 | ppl   648.44\n",
      "| epoch   6 |    81/   91 batches | lr 10.00 | ms/batch 4219.03 | loss  6.60 | ppl   734.36\n",
      "| epoch   6 |    82/   91 batches | lr 10.00 | ms/batch 4030.92 | loss  6.57 | ppl   712.38\n",
      "| epoch   6 |    83/   91 batches | lr 10.00 | ms/batch 3960.40 | loss  6.48 | ppl   653.87\n",
      "| epoch   6 |    84/   91 batches | lr 10.00 | ms/batch 3879.42 | loss  6.37 | ppl   586.85\n",
      "| epoch   6 |    85/   91 batches | lr 10.00 | ms/batch 3910.58 | loss  6.39 | ppl   598.13\n",
      "| epoch   6 |    86/   91 batches | lr 10.00 | ms/batch 3888.15 | loss  6.30 | ppl   545.73\n",
      "| epoch   6 |    87/   91 batches | lr 10.00 | ms/batch 3901.15 | loss  6.33 | ppl   558.39\n",
      "| epoch   6 |    88/   91 batches | lr 10.00 | ms/batch 3972.57 | loss  6.41 | ppl   608.43\n",
      "| epoch   6 |    89/   91 batches | lr 10.00 | ms/batch 3830.95 | loss  6.60 | ppl   734.22\n",
      "| epoch   6 |    90/   91 batches | lr 10.00 | ms/batch 3897.33 | loss  6.34 | ppl   565.01\n",
      "| epoch   6 |    91/   91 batches | lr 10.00 | ms/batch 2953.88 | loss  6.44 | ppl   626.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 405.36s | valid loss  0.21 | valid ppl     1.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |     1/   91 batches | lr 10.00 | ms/batch 8103.81 | loss 13.05 | ppl 463979.43\n",
      "| epoch   7 |     2/   91 batches | lr 10.00 | ms/batch 3972.26 | loss  6.34 | ppl   567.29\n",
      "| epoch   7 |     3/   91 batches | lr 10.00 | ms/batch 3945.10 | loss  6.20 | ppl   490.96\n",
      "| epoch   7 |     4/   91 batches | lr 10.00 | ms/batch 3872.50 | loss  6.34 | ppl   565.99\n",
      "| epoch   7 |     5/   91 batches | lr 10.00 | ms/batch 3917.91 | loss  6.35 | ppl   575.30\n",
      "| epoch   7 |     6/   91 batches | lr 10.00 | ms/batch 3989.21 | loss  6.42 | ppl   614.53\n",
      "| epoch   7 |     7/   91 batches | lr 10.00 | ms/batch 3962.12 | loss  6.49 | ppl   657.31\n",
      "| epoch   7 |     8/   91 batches | lr 10.00 | ms/batch 3953.29 | loss  6.41 | ppl   608.33\n",
      "| epoch   7 |     9/   91 batches | lr 10.00 | ms/batch 4112.11 | loss  6.38 | ppl   589.87\n",
      "| epoch   7 |    10/   91 batches | lr 10.00 | ms/batch 4012.83 | loss  6.30 | ppl   542.70\n",
      "| epoch   7 |    11/   91 batches | lr 10.00 | ms/batch 4092.94 | loss  6.36 | ppl   578.71\n",
      "| epoch   7 |    12/   91 batches | lr 10.00 | ms/batch 3916.33 | loss  6.49 | ppl   657.09\n",
      "| epoch   7 |    13/   91 batches | lr 10.00 | ms/batch 3985.59 | loss  6.33 | ppl   562.74\n",
      "| epoch   7 |    14/   91 batches | lr 10.00 | ms/batch 3943.00 | loss  6.48 | ppl   655.11\n",
      "| epoch   7 |    15/   91 batches | lr 10.00 | ms/batch 5240.84 | loss  6.39 | ppl   594.97\n",
      "| epoch   7 |    16/   91 batches | lr 10.00 | ms/batch 4426.05 | loss  6.33 | ppl   559.79\n",
      "| epoch   7 |    17/   91 batches | lr 10.00 | ms/batch 4758.69 | loss  6.31 | ppl   550.89\n",
      "| epoch   7 |    18/   91 batches | lr 10.00 | ms/batch 3889.00 | loss  6.38 | ppl   590.81\n",
      "| epoch   7 |    19/   91 batches | lr 10.00 | ms/batch 3994.38 | loss  6.34 | ppl   565.99\n",
      "| epoch   7 |    20/   91 batches | lr 10.00 | ms/batch 3924.31 | loss  6.53 | ppl   687.10\n",
      "| epoch   7 |    21/   91 batches | lr 10.00 | ms/batch 3860.68 | loss  6.29 | ppl   541.68\n",
      "| epoch   7 |    22/   91 batches | lr 10.00 | ms/batch 3959.03 | loss  6.31 | ppl   549.86\n",
      "| epoch   7 |    23/   91 batches | lr 10.00 | ms/batch 3956.49 | loss  6.44 | ppl   623.50\n",
      "| epoch   7 |    24/   91 batches | lr 10.00 | ms/batch 3840.32 | loss  6.27 | ppl   530.37\n",
      "| epoch   7 |    25/   91 batches | lr 10.00 | ms/batch 3969.27 | loss  6.39 | ppl   595.66\n",
      "| epoch   7 |    26/   91 batches | lr 10.00 | ms/batch 3904.65 | loss  6.30 | ppl   542.37\n",
      "| epoch   7 |    27/   91 batches | lr 10.00 | ms/batch 4053.34 | loss  6.45 | ppl   633.92\n",
      "| epoch   7 |    28/   91 batches | lr 10.00 | ms/batch 5173.85 | loss  6.36 | ppl   578.05\n",
      "| epoch   7 |    29/   91 batches | lr 10.00 | ms/batch 3965.90 | loss  6.36 | ppl   579.55\n",
      "| epoch   7 |    30/   91 batches | lr 10.00 | ms/batch 3972.79 | loss  6.32 | ppl   556.21\n",
      "| epoch   7 |    31/   91 batches | lr 10.00 | ms/batch 4010.22 | loss  6.53 | ppl   682.94\n",
      "| epoch   7 |    32/   91 batches | lr 10.00 | ms/batch 3920.68 | loss  6.44 | ppl   626.27\n",
      "| epoch   7 |    33/   91 batches | lr 10.00 | ms/batch 3877.12 | loss  6.37 | ppl   584.19\n",
      "| epoch   7 |    34/   91 batches | lr 10.00 | ms/batch 3904.81 | loss  6.30 | ppl   545.27\n",
      "| epoch   7 |    35/   91 batches | lr 10.00 | ms/batch 3927.99 | loss  6.33 | ppl   561.08\n",
      "| epoch   7 |    36/   91 batches | lr 10.00 | ms/batch 3938.35 | loss  6.32 | ppl   555.18\n",
      "| epoch   7 |    37/   91 batches | lr 10.00 | ms/batch 3909.72 | loss  6.36 | ppl   575.47\n",
      "| epoch   7 |    38/   91 batches | lr 10.00 | ms/batch 3929.80 | loss  6.42 | ppl   614.08\n",
      "| epoch   7 |    39/   91 batches | lr 10.00 | ms/batch 3887.36 | loss  6.38 | ppl   591.77\n",
      "| epoch   7 |    40/   91 batches | lr 10.00 | ms/batch 3914.57 | loss  6.31 | ppl   548.97\n",
      "| epoch   7 |    41/   91 batches | lr 10.00 | ms/batch 4080.95 | loss  6.27 | ppl   529.24\n",
      "| epoch   7 |    42/   91 batches | lr 10.00 | ms/batch 4049.03 | loss  6.41 | ppl   609.89\n",
      "| epoch   7 |    43/   91 batches | lr 10.00 | ms/batch 4025.40 | loss  6.26 | ppl   520.93\n",
      "| epoch   7 |    44/   91 batches | lr 10.00 | ms/batch 3995.74 | loss  6.36 | ppl   575.70\n",
      "| epoch   7 |    45/   91 batches | lr 10.00 | ms/batch 3921.89 | loss  6.35 | ppl   571.95\n",
      "| epoch   7 |    46/   91 batches | lr 10.00 | ms/batch 3918.56 | loss  6.26 | ppl   524.40\n",
      "| epoch   7 |    47/   91 batches | lr 10.00 | ms/batch 3961.07 | loss  6.47 | ppl   642.77\n",
      "| epoch   7 |    48/   91 batches | lr 10.00 | ms/batch 4007.42 | loss  6.24 | ppl   514.07\n",
      "| epoch   7 |    49/   91 batches | lr 10.00 | ms/batch 3862.83 | loss  6.36 | ppl   580.99\n",
      "| epoch   7 |    50/   91 batches | lr 10.00 | ms/batch 4056.52 | loss  6.29 | ppl   537.59\n",
      "| epoch   7 |    51/   91 batches | lr 10.00 | ms/batch 3874.08 | loss  6.38 | ppl   590.94\n",
      "| epoch   7 |    52/   91 batches | lr 10.00 | ms/batch 3950.91 | loss  6.24 | ppl   511.10\n",
      "| epoch   7 |    53/   91 batches | lr 10.00 | ms/batch 4293.44 | loss  6.41 | ppl   607.04\n",
      "| epoch   7 |    54/   91 batches | lr 10.00 | ms/batch 4404.78 | loss  6.45 | ppl   635.37\n",
      "| epoch   7 |    55/   91 batches | lr 10.00 | ms/batch 4367.33 | loss  6.37 | ppl   585.87\n",
      "| epoch   7 |    56/   91 batches | lr 10.00 | ms/batch 3966.03 | loss  6.41 | ppl   609.43\n",
      "| epoch   7 |    57/   91 batches | lr 10.00 | ms/batch 4048.32 | loss  6.34 | ppl   569.44\n",
      "| epoch   7 |    58/   91 batches | lr 10.00 | ms/batch 3962.87 | loss  6.44 | ppl   623.82\n",
      "| epoch   7 |    59/   91 batches | lr 10.00 | ms/batch 3925.88 | loss  6.33 | ppl   561.68\n",
      "| epoch   7 |    60/   91 batches | lr 10.00 | ms/batch 3896.27 | loss  6.28 | ppl   535.79\n",
      "| epoch   7 |    61/   91 batches | lr 10.00 | ms/batch 3950.15 | loss  6.41 | ppl   606.50\n",
      "| epoch   7 |    62/   91 batches | lr 10.00 | ms/batch 4007.60 | loss  6.39 | ppl   595.40\n",
      "| epoch   7 |    63/   91 batches | lr 10.00 | ms/batch 3970.96 | loss  6.24 | ppl   514.33\n",
      "| epoch   7 |    64/   91 batches | lr 10.00 | ms/batch 3827.97 | loss  6.31 | ppl   549.51\n",
      "| epoch   7 |    65/   91 batches | lr 10.00 | ms/batch 3962.69 | loss  6.33 | ppl   562.86\n",
      "| epoch   7 |    66/   91 batches | lr 10.00 | ms/batch 3938.20 | loss  6.28 | ppl   531.62\n",
      "| epoch   7 |    67/   91 batches | lr 10.00 | ms/batch 3891.30 | loss  6.26 | ppl   521.76\n",
      "| epoch   7 |    68/   91 batches | lr 10.00 | ms/batch 3998.03 | loss  6.35 | ppl   572.33\n",
      "| epoch   7 |    69/   91 batches | lr 10.00 | ms/batch 3954.21 | loss  6.36 | ppl   578.76\n",
      "| epoch   7 |    70/   91 batches | lr 10.00 | ms/batch 3927.99 | loss  6.26 | ppl   522.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |    71/   91 batches | lr 10.00 | ms/batch 3939.94 | loss  6.17 | ppl   480.44\n",
      "| epoch   7 |    72/   91 batches | lr 10.00 | ms/batch 3995.14 | loss  6.33 | ppl   562.90\n",
      "| epoch   7 |    73/   91 batches | lr 10.00 | ms/batch 4076.17 | loss  6.33 | ppl   558.60\n",
      "| epoch   7 |    74/   91 batches | lr 10.00 | ms/batch 3987.90 | loss  6.40 | ppl   602.88\n",
      "| epoch   7 |    75/   91 batches | lr 10.00 | ms/batch 3982.23 | loss  6.37 | ppl   586.97\n",
      "| epoch   7 |    76/   91 batches | lr 10.00 | ms/batch 3954.85 | loss  6.19 | ppl   490.08\n",
      "| epoch   7 |    77/   91 batches | lr 10.00 | ms/batch 3892.79 | loss  6.22 | ppl   504.30\n",
      "| epoch   7 |    78/   91 batches | lr 10.00 | ms/batch 3926.36 | loss  6.39 | ppl   592.97\n",
      "| epoch   7 |    79/   91 batches | lr 10.00 | ms/batch 3915.02 | loss  6.39 | ppl   592.95\n",
      "| epoch   7 |    80/   91 batches | lr 10.00 | ms/batch 3929.81 | loss  6.44 | ppl   628.88\n",
      "| epoch   7 |    81/   91 batches | lr 10.00 | ms/batch 4005.03 | loss  6.42 | ppl   616.49\n",
      "| epoch   7 |    82/   91 batches | lr 10.00 | ms/batch 3937.08 | loss  6.39 | ppl   593.44\n",
      "| epoch   7 |    83/   91 batches | lr 10.00 | ms/batch 3902.13 | loss  6.34 | ppl   566.08\n",
      "| epoch   7 |    84/   91 batches | lr 10.00 | ms/batch 3944.52 | loss  6.32 | ppl   554.02\n",
      "| epoch   7 |    85/   91 batches | lr 10.00 | ms/batch 3926.77 | loss  6.23 | ppl   506.44\n",
      "| epoch   7 |    86/   91 batches | lr 10.00 | ms/batch 3870.38 | loss  6.24 | ppl   514.32\n",
      "| epoch   7 |    87/   91 batches | lr 10.00 | ms/batch 3934.73 | loss  6.24 | ppl   514.35\n",
      "| epoch   7 |    88/   91 batches | lr 10.00 | ms/batch 4008.32 | loss  6.34 | ppl   565.03\n",
      "| epoch   7 |    89/   91 batches | lr 10.00 | ms/batch 3877.57 | loss  6.52 | ppl   678.40\n",
      "| epoch   7 |    90/   91 batches | lr 10.00 | ms/batch 4016.07 | loss  6.31 | ppl   551.99\n",
      "| epoch   7 |    91/   91 batches | lr 10.00 | ms/batch 2910.06 | loss  6.33 | ppl   562.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 394.88s | valid loss  0.21 | valid ppl     1.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |     1/   91 batches | lr 10.00 | ms/batch 7893.33 | loss 12.77 | ppl 351591.76\n",
      "| epoch   8 |     2/   91 batches | lr 10.00 | ms/batch 3883.45 | loss  6.24 | ppl   512.53\n",
      "| epoch   8 |     3/   91 batches | lr 10.00 | ms/batch 3917.81 | loss  6.14 | ppl   462.19\n",
      "| epoch   8 |     4/   91 batches | lr 10.00 | ms/batch 3950.04 | loss  6.26 | ppl   520.91\n",
      "| epoch   8 |     5/   91 batches | lr 10.00 | ms/batch 3970.08 | loss  6.27 | ppl   528.84\n",
      "| epoch   8 |     6/   91 batches | lr 10.00 | ms/batch 4067.17 | loss  6.41 | ppl   609.26\n",
      "| epoch   8 |     7/   91 batches | lr 10.00 | ms/batch 3940.33 | loss  6.25 | ppl   520.16\n",
      "| epoch   8 |     8/   91 batches | lr 10.00 | ms/batch 3862.48 | loss  6.25 | ppl   516.09\n",
      "| epoch   8 |     9/   91 batches | lr 10.00 | ms/batch 3885.52 | loss  6.27 | ppl   528.37\n",
      "| epoch   8 |    10/   91 batches | lr 10.00 | ms/batch 3753.42 | loss  6.21 | ppl   497.06\n",
      "| epoch   8 |    11/   91 batches | lr 10.00 | ms/batch 3939.69 | loss  6.41 | ppl   606.72\n",
      "| epoch   8 |    12/   91 batches | lr 10.00 | ms/batch 3934.23 | loss  6.44 | ppl   626.18\n",
      "| epoch   8 |    13/   91 batches | lr 10.00 | ms/batch 3872.61 | loss  6.26 | ppl   523.76\n",
      "| epoch   8 |    14/   91 batches | lr 10.00 | ms/batch 3953.79 | loss  6.34 | ppl   568.97\n",
      "| epoch   8 |    15/   91 batches | lr 10.00 | ms/batch 3859.60 | loss  6.33 | ppl   558.84\n",
      "| epoch   8 |    16/   91 batches | lr 10.00 | ms/batch 3930.12 | loss  6.28 | ppl   535.44\n",
      "| epoch   8 |    17/   91 batches | lr 10.00 | ms/batch 3806.88 | loss  6.23 | ppl   505.85\n",
      "| epoch   8 |    18/   91 batches | lr 10.00 | ms/batch 3824.88 | loss  6.29 | ppl   538.00\n",
      "| epoch   8 |    19/   91 batches | lr 10.00 | ms/batch 3810.04 | loss  6.27 | ppl   530.54\n",
      "| epoch   8 |    20/   91 batches | lr 10.00 | ms/batch 3950.35 | loss  6.34 | ppl   569.64\n",
      "| epoch   8 |    21/   91 batches | lr 10.00 | ms/batch 3888.00 | loss  6.26 | ppl   522.94\n",
      "| epoch   8 |    22/   91 batches | lr 10.00 | ms/batch 3951.15 | loss  6.19 | ppl   488.25\n",
      "| epoch   8 |    23/   91 batches | lr 10.00 | ms/batch 3871.73 | loss  6.41 | ppl   607.46\n",
      "| epoch   8 |    24/   91 batches | lr 10.00 | ms/batch 3778.71 | loss  6.09 | ppl   443.18\n",
      "| epoch   8 |    25/   91 batches | lr 10.00 | ms/batch 3981.63 | loss  6.21 | ppl   498.88\n",
      "| epoch   8 |    26/   91 batches | lr 10.00 | ms/batch 3903.81 | loss  6.17 | ppl   476.14\n",
      "| epoch   8 |    27/   91 batches | lr 10.00 | ms/batch 3850.62 | loss  6.34 | ppl   568.62\n",
      "| epoch   8 |    28/   91 batches | lr 10.00 | ms/batch 3913.72 | loss  6.30 | ppl   546.81\n",
      "| epoch   8 |    29/   91 batches | lr 10.00 | ms/batch 3887.81 | loss  6.29 | ppl   538.70\n",
      "| epoch   8 |    30/   91 batches | lr 10.00 | ms/batch 3964.22 | loss  6.25 | ppl   517.13\n",
      "| epoch   8 |    31/   91 batches | lr 10.00 | ms/batch 3906.84 | loss  6.36 | ppl   579.67\n",
      "| epoch   8 |    32/   91 batches | lr 10.00 | ms/batch 3897.06 | loss  6.29 | ppl   540.51\n",
      "| epoch   8 |    33/   91 batches | lr 10.00 | ms/batch 4001.78 | loss  6.34 | ppl   564.74\n",
      "| epoch   8 |    34/   91 batches | lr 10.00 | ms/batch 4178.04 | loss  6.23 | ppl   509.59\n",
      "| epoch   8 |    35/   91 batches | lr 10.00 | ms/batch 4050.65 | loss  6.21 | ppl   498.50\n",
      "| epoch   8 |    36/   91 batches | lr 10.00 | ms/batch 3926.93 | loss  6.28 | ppl   535.56\n",
      "| epoch   8 |    37/   91 batches | lr 10.00 | ms/batch 3935.16 | loss  6.23 | ppl   506.49\n",
      "| epoch   8 |    38/   91 batches | lr 10.00 | ms/batch 3956.39 | loss  6.22 | ppl   502.51\n",
      "| epoch   8 |    39/   91 batches | lr 10.00 | ms/batch 3902.88 | loss  6.35 | ppl   571.24\n",
      "| epoch   8 |    40/   91 batches | lr 10.00 | ms/batch 4081.43 | loss  6.26 | ppl   522.07\n",
      "| epoch   8 |    41/   91 batches | lr 10.00 | ms/batch 3900.12 | loss  6.26 | ppl   522.73\n",
      "| epoch   8 |    42/   91 batches | lr 10.00 | ms/batch 3834.25 | loss  6.35 | ppl   571.59\n",
      "| epoch   8 |    43/   91 batches | lr 10.00 | ms/batch 3871.66 | loss  6.12 | ppl   455.21\n",
      "| epoch   8 |    44/   91 batches | lr 10.00 | ms/batch 3758.99 | loss  6.13 | ppl   457.22\n",
      "| epoch   8 |    45/   91 batches | lr 10.00 | ms/batch 3967.64 | loss  6.30 | ppl   546.93\n",
      "| epoch   8 |    46/   91 batches | lr 10.00 | ms/batch 3922.57 | loss  6.15 | ppl   468.15\n",
      "| epoch   8 |    47/   91 batches | lr 10.00 | ms/batch 3884.53 | loss  6.28 | ppl   534.14\n",
      "| epoch   8 |    48/   91 batches | lr 10.00 | ms/batch 3874.52 | loss  6.18 | ppl   485.38\n",
      "| epoch   8 |    49/   91 batches | lr 10.00 | ms/batch 3888.87 | loss  6.33 | ppl   558.95\n",
      "| epoch   8 |    50/   91 batches | lr 10.00 | ms/batch 3745.15 | loss  6.23 | ppl   508.80\n",
      "| epoch   8 |    51/   91 batches | lr 10.00 | ms/batch 3901.39 | loss  6.40 | ppl   601.80\n",
      "| epoch   8 |    52/   91 batches | lr 10.00 | ms/batch 3948.83 | loss  6.14 | ppl   465.89\n",
      "| epoch   8 |    53/   91 batches | lr 10.00 | ms/batch 3915.41 | loss  6.21 | ppl   499.41\n",
      "| epoch   8 |    54/   91 batches | lr 10.00 | ms/batch 3855.70 | loss  6.30 | ppl   543.53\n",
      "| epoch   8 |    55/   91 batches | lr 10.00 | ms/batch 3854.01 | loss  6.35 | ppl   574.57\n",
      "| epoch   8 |    56/   91 batches | lr 10.00 | ms/batch 3934.18 | loss  6.47 | ppl   647.53\n",
      "| epoch   8 |    57/   91 batches | lr 10.00 | ms/batch 3909.73 | loss  6.29 | ppl   541.19\n",
      "| epoch   8 |    58/   91 batches | lr 10.00 | ms/batch 3901.67 | loss  6.32 | ppl   556.78\n",
      "| epoch   8 |    59/   91 batches | lr 10.00 | ms/batch 3866.37 | loss  6.18 | ppl   480.61\n",
      "| epoch   8 |    60/   91 batches | lr 10.00 | ms/batch 3969.14 | loss  6.19 | ppl   487.13\n",
      "| epoch   8 |    61/   91 batches | lr 10.00 | ms/batch 3948.59 | loss  6.31 | ppl   548.28\n",
      "| epoch   8 |    62/   91 batches | lr 10.00 | ms/batch 3831.13 | loss  6.27 | ppl   529.90\n",
      "| epoch   8 |    63/   91 batches | lr 10.00 | ms/batch 3885.11 | loss  6.10 | ppl   446.26\n",
      "| epoch   8 |    64/   91 batches | lr 10.00 | ms/batch 3874.11 | loss  6.20 | ppl   493.91\n",
      "| epoch   8 |    65/   91 batches | lr 10.00 | ms/batch 3865.43 | loss  6.23 | ppl   507.35\n",
      "| epoch   8 |    66/   91 batches | lr 10.00 | ms/batch 4086.44 | loss  6.23 | ppl   506.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |    67/   91 batches | lr 10.00 | ms/batch 3957.96 | loss  6.16 | ppl   471.34\n",
      "| epoch   8 |    68/   91 batches | lr 10.00 | ms/batch 3898.23 | loss  6.15 | ppl   468.37\n",
      "| epoch   8 |    69/   91 batches | lr 10.00 | ms/batch 3891.49 | loss  6.19 | ppl   486.55\n",
      "| epoch   8 |    70/   91 batches | lr 10.00 | ms/batch 4175.19 | loss  6.16 | ppl   473.39\n",
      "| epoch   8 |    71/   91 batches | lr 10.00 | ms/batch 4057.68 | loss  6.19 | ppl   486.12\n",
      "| epoch   8 |    72/   91 batches | lr 10.00 | ms/batch 3826.57 | loss  6.26 | ppl   521.57\n",
      "| epoch   8 |    73/   91 batches | lr 10.00 | ms/batch 3938.01 | loss  6.29 | ppl   539.25\n",
      "| epoch   8 |    74/   91 batches | lr 10.00 | ms/batch 3819.53 | loss  6.23 | ppl   507.02\n",
      "| epoch   8 |    75/   91 batches | lr 10.00 | ms/batch 3874.68 | loss  6.30 | ppl   546.38\n",
      "| epoch   8 |    76/   91 batches | lr 10.00 | ms/batch 3898.45 | loss  6.08 | ppl   438.63\n",
      "| epoch   8 |    77/   91 batches | lr 10.00 | ms/batch 3834.16 | loss  6.12 | ppl   452.71\n",
      "| epoch   8 |    78/   91 batches | lr 10.00 | ms/batch 3898.80 | loss  6.28 | ppl   531.89\n",
      "| epoch   8 |    79/   91 batches | lr 10.00 | ms/batch 3933.05 | loss  6.29 | ppl   539.11\n",
      "| epoch   8 |    80/   91 batches | lr 10.00 | ms/batch 3904.41 | loss  6.28 | ppl   536.24\n",
      "| epoch   8 |    81/   91 batches | lr 10.00 | ms/batch 3941.26 | loss  6.40 | ppl   602.31\n",
      "| epoch   8 |    82/   91 batches | lr 10.00 | ms/batch 3873.89 | loss  6.28 | ppl   535.55\n",
      "| epoch   8 |    83/   91 batches | lr 10.00 | ms/batch 3951.69 | loss  6.28 | ppl   535.86\n",
      "| epoch   8 |    84/   91 batches | lr 10.00 | ms/batch 3866.75 | loss  6.22 | ppl   503.69\n",
      "| epoch   8 |    85/   91 batches | lr 10.00 | ms/batch 3898.06 | loss  6.13 | ppl   460.32\n",
      "| epoch   8 |    86/   91 batches | lr 10.00 | ms/batch 3955.72 | loss  6.14 | ppl   465.22\n",
      "| epoch   8 |    87/   91 batches | lr 10.00 | ms/batch 3816.66 | loss  6.14 | ppl   463.97\n",
      "| epoch   8 |    88/   91 batches | lr 10.00 | ms/batch 3792.59 | loss  6.19 | ppl   487.47\n",
      "| epoch   8 |    89/   91 batches | lr 10.00 | ms/batch 3779.78 | loss  6.24 | ppl   511.25\n",
      "| epoch   8 |    90/   91 batches | lr 10.00 | ms/batch 3856.02 | loss  6.16 | ppl   472.49\n",
      "| epoch   8 |    91/   91 batches | lr 10.00 | ms/batch 2869.59 | loss  6.23 | ppl   509.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 385.15s | valid loss  0.21 | valid ppl     1.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |     1/   91 batches | lr 10.00 | ms/batch 7784.38 | loss 12.64 | ppl 308735.06\n",
      "| epoch   9 |     2/   91 batches | lr 10.00 | ms/batch 3909.30 | loss  6.14 | ppl   463.42\n",
      "| epoch   9 |     3/   91 batches | lr 10.00 | ms/batch 4009.42 | loss  6.09 | ppl   443.48\n",
      "| epoch   9 |     4/   91 batches | lr 10.00 | ms/batch 3909.80 | loss  6.21 | ppl   499.01\n",
      "| epoch   9 |     5/   91 batches | lr 10.00 | ms/batch 3913.27 | loss  6.21 | ppl   497.60\n",
      "| epoch   9 |     6/   91 batches | lr 10.00 | ms/batch 3779.02 | loss  6.38 | ppl   588.68\n",
      "| epoch   9 |     7/   91 batches | lr 10.00 | ms/batch 3960.80 | loss  6.18 | ppl   484.10\n",
      "| epoch   9 |     8/   91 batches | lr 10.00 | ms/batch 3891.45 | loss  6.14 | ppl   466.09\n",
      "| epoch   9 |     9/   91 batches | lr 10.00 | ms/batch 3888.26 | loss  6.18 | ppl   482.50\n",
      "| epoch   9 |    10/   91 batches | lr 10.00 | ms/batch 3857.17 | loss  6.11 | ppl   450.45\n",
      "| epoch   9 |    11/   91 batches | lr 10.00 | ms/batch 3906.02 | loss  6.24 | ppl   513.94\n",
      "| epoch   9 |    12/   91 batches | lr 10.00 | ms/batch 3913.87 | loss  6.26 | ppl   522.71\n",
      "| epoch   9 |    13/   91 batches | lr 10.00 | ms/batch 3858.92 | loss  6.13 | ppl   460.00\n",
      "| epoch   9 |    14/   91 batches | lr 10.00 | ms/batch 4039.54 | loss  6.27 | ppl   526.30\n",
      "| epoch   9 |    15/   91 batches | lr 10.00 | ms/batch 4003.26 | loss  6.20 | ppl   493.52\n",
      "| epoch   9 |    16/   91 batches | lr 10.00 | ms/batch 3765.26 | loss  6.19 | ppl   487.68\n",
      "| epoch   9 |    17/   91 batches | lr 10.00 | ms/batch 3704.34 | loss  6.21 | ppl   496.38\n",
      "| epoch   9 |    18/   91 batches | lr 10.00 | ms/batch 3996.78 | loss  6.17 | ppl   479.09\n",
      "| epoch   9 |    19/   91 batches | lr 10.00 | ms/batch 3838.96 | loss  6.20 | ppl   493.81\n",
      "| epoch   9 |    20/   91 batches | lr 10.00 | ms/batch 3721.11 | loss  6.22 | ppl   503.85\n",
      "| epoch   9 |    21/   91 batches | lr 10.00 | ms/batch 3840.33 | loss  6.09 | ppl   440.46\n",
      "| epoch   9 |    22/   91 batches | lr 10.00 | ms/batch 3948.61 | loss  6.14 | ppl   463.12\n",
      "| epoch   9 |    23/   91 batches | lr 10.00 | ms/batch 3937.23 | loss  6.29 | ppl   536.50\n",
      "| epoch   9 |    24/   91 batches | lr 10.00 | ms/batch 3760.55 | loss  6.12 | ppl   453.20\n",
      "| epoch   9 |    25/   91 batches | lr 10.00 | ms/batch 4007.73 | loss  6.19 | ppl   487.17\n",
      "| epoch   9 |    26/   91 batches | lr 10.00 | ms/batch 3925.05 | loss  6.10 | ppl   447.67\n",
      "| epoch   9 |    27/   91 batches | lr 10.00 | ms/batch 3885.10 | loss  6.34 | ppl   568.11\n",
      "| epoch   9 |    28/   91 batches | lr 10.00 | ms/batch 3825.34 | loss  6.39 | ppl   595.01\n",
      "| epoch   9 |    29/   91 batches | lr 10.00 | ms/batch 4187.84 | loss  6.16 | ppl   475.52\n",
      "| epoch   9 |    30/   91 batches | lr 10.00 | ms/batch 4038.31 | loss  6.10 | ppl   446.33\n",
      "| epoch   9 |    31/   91 batches | lr 10.00 | ms/batch 3920.30 | loss  6.27 | ppl   526.07\n",
      "| epoch   9 |    32/   91 batches | lr 10.00 | ms/batch 3841.94 | loss  6.27 | ppl   526.61\n",
      "| epoch   9 |    33/   91 batches | lr 10.00 | ms/batch 3891.39 | loss  6.19 | ppl   487.01\n",
      "| epoch   9 |    34/   91 batches | lr 10.00 | ms/batch 3965.61 | loss  6.14 | ppl   462.14\n",
      "| epoch   9 |    35/   91 batches | lr 10.00 | ms/batch 3752.05 | loss  6.13 | ppl   461.39\n",
      "| epoch   9 |    36/   91 batches | lr 10.00 | ms/batch 3966.65 | loss  6.19 | ppl   489.28\n",
      "| epoch   9 |    37/   91 batches | lr 10.00 | ms/batch 3801.01 | loss  6.22 | ppl   502.40\n",
      "| epoch   9 |    38/   91 batches | lr 10.00 | ms/batch 3927.85 | loss  6.18 | ppl   480.64\n",
      "| epoch   9 |    39/   91 batches | lr 10.00 | ms/batch 3924.24 | loss  6.21 | ppl   500.02\n",
      "| epoch   9 |    40/   91 batches | lr 10.00 | ms/batch 3918.46 | loss  6.17 | ppl   475.96\n",
      "| epoch   9 |    41/   91 batches | lr 10.00 | ms/batch 3840.16 | loss  6.21 | ppl   495.69\n",
      "| epoch   9 |    42/   91 batches | lr 10.00 | ms/batch 3892.42 | loss  6.27 | ppl   526.46\n",
      "| epoch   9 |    43/   91 batches | lr 10.00 | ms/batch 3923.25 | loss  6.06 | ppl   429.48\n",
      "| epoch   9 |    44/   91 batches | lr 10.00 | ms/batch 3912.25 | loss  6.06 | ppl   430.15\n",
      "| epoch   9 |    45/   91 batches | lr 10.00 | ms/batch 3904.24 | loss  6.25 | ppl   516.48\n",
      "| epoch   9 |    46/   91 batches | lr 10.00 | ms/batch 3933.80 | loss  6.07 | ppl   434.21\n",
      "| epoch   9 |    47/   91 batches | lr 10.00 | ms/batch 3993.89 | loss  6.17 | ppl   476.54\n",
      "| epoch   9 |    48/   91 batches | lr 10.00 | ms/batch 3865.18 | loss  6.05 | ppl   424.97\n",
      "| epoch   9 |    49/   91 batches | lr 10.00 | ms/batch 3923.67 | loss  6.17 | ppl   476.77\n",
      "| epoch   9 |    50/   91 batches | lr 10.00 | ms/batch 4320.93 | loss  6.19 | ppl   488.21\n",
      "| epoch   9 |    51/   91 batches | lr 10.00 | ms/batch 3889.84 | loss  6.30 | ppl   546.94\n",
      "| epoch   9 |    52/   91 batches | lr 10.00 | ms/batch 3886.12 | loss  6.05 | ppl   425.29\n",
      "| epoch   9 |    53/   91 batches | lr 10.00 | ms/batch 3967.35 | loss  6.10 | ppl   444.44\n",
      "| epoch   9 |    54/   91 batches | lr 10.00 | ms/batch 4007.71 | loss  6.21 | ppl   498.52\n",
      "| epoch   9 |    55/   91 batches | lr 10.00 | ms/batch 3982.73 | loss  6.25 | ppl   519.95\n",
      "| epoch   9 |    56/   91 batches | lr 10.00 | ms/batch 3881.00 | loss  6.40 | ppl   603.15\n",
      "| epoch   9 |    57/   91 batches | lr 10.00 | ms/batch 3810.65 | loss  6.20 | ppl   494.35\n",
      "| epoch   9 |    58/   91 batches | lr 10.00 | ms/batch 3818.60 | loss  6.22 | ppl   503.42\n",
      "| epoch   9 |    59/   91 batches | lr 10.00 | ms/batch 4023.29 | loss  6.04 | ppl   420.96\n",
      "| epoch   9 |    60/   91 batches | lr 10.00 | ms/batch 3833.47 | loss  6.09 | ppl   439.54\n",
      "| epoch   9 |    61/   91 batches | lr 10.00 | ms/batch 3950.12 | loss  6.21 | ppl   498.58\n",
      "| epoch   9 |    62/   91 batches | lr 10.00 | ms/batch 3963.19 | loss  6.09 | ppl   441.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |    63/   91 batches | lr 10.00 | ms/batch 3891.10 | loss  6.09 | ppl   443.14\n",
      "| epoch   9 |    64/   91 batches | lr 10.00 | ms/batch 3953.34 | loss  6.17 | ppl   478.17\n",
      "| epoch   9 |    65/   91 batches | lr 10.00 | ms/batch 3944.18 | loss  6.21 | ppl   496.42\n",
      "| epoch   9 |    66/   91 batches | lr 10.00 | ms/batch 3897.55 | loss  6.11 | ppl   452.13\n",
      "| epoch   9 |    67/   91 batches | lr 10.00 | ms/batch 3952.28 | loss  6.08 | ppl   437.26\n",
      "| epoch   9 |    68/   91 batches | lr 10.00 | ms/batch 3913.09 | loss  6.14 | ppl   464.46\n",
      "| epoch   9 |    69/   91 batches | lr 10.00 | ms/batch 3882.72 | loss  6.10 | ppl   446.90\n",
      "| epoch   9 |    70/   91 batches | lr 10.00 | ms/batch 3925.82 | loss  6.08 | ppl   438.52\n",
      "| epoch   9 |    71/   91 batches | lr 10.00 | ms/batch 3916.68 | loss  6.03 | ppl   414.78\n",
      "| epoch   9 |    72/   91 batches | lr 10.00 | ms/batch 3885.67 | loss  6.13 | ppl   458.29\n",
      "| epoch   9 |    73/   91 batches | lr 10.00 | ms/batch 3727.26 | loss  6.24 | ppl   510.72\n",
      "| epoch   9 |    74/   91 batches | lr 10.00 | ms/batch 4246.17 | loss  6.14 | ppl   465.67\n",
      "| epoch   9 |    75/   91 batches | lr 10.00 | ms/batch 4601.69 | loss  6.24 | ppl   514.80\n",
      "| epoch   9 |    76/   91 batches | lr 10.00 | ms/batch 129695.27 | loss  6.00 | ppl   404.92\n",
      "| epoch   9 |    77/   91 batches | lr 10.00 | ms/batch 5300.59 | loss  6.02 | ppl   413.17\n",
      "| epoch   9 |    78/   91 batches | lr 10.00 | ms/batch 4320.80 | loss  6.18 | ppl   485.23\n",
      "| epoch   9 |    79/   91 batches | lr 10.00 | ms/batch 4012.24 | loss  6.11 | ppl   448.76\n",
      "| epoch   9 |    80/   91 batches | lr 10.00 | ms/batch 4026.06 | loss  6.20 | ppl   494.54\n",
      "| epoch   9 |    81/   91 batches | lr 10.00 | ms/batch 4072.01 | loss  6.31 | ppl   548.37\n",
      "| epoch   9 |    82/   91 batches | lr 10.00 | ms/batch 4767.70 | loss  6.32 | ppl   557.22\n",
      "| epoch   9 |    83/   91 batches | lr 10.00 | ms/batch 3785.64 | loss  6.19 | ppl   489.83\n",
      "| epoch   9 |    84/   91 batches | lr 10.00 | ms/batch 4066.12 | loss  6.15 | ppl   470.65\n",
      "| epoch   9 |    85/   91 batches | lr 10.00 | ms/batch 3794.68 | loss  6.07 | ppl   432.71\n",
      "| epoch   9 |    86/   91 batches | lr 10.00 | ms/batch 4180.70 | loss  6.09 | ppl   441.21\n",
      "| epoch   9 |    87/   91 batches | lr 10.00 | ms/batch 3788.64 | loss  6.15 | ppl   467.86\n",
      "| epoch   9 |    88/   91 batches | lr 10.00 | ms/batch 3888.01 | loss  6.15 | ppl   471.07\n",
      "| epoch   9 |    89/   91 batches | lr 10.00 | ms/batch 3851.73 | loss  6.26 | ppl   520.98\n",
      "| epoch   9 |    90/   91 batches | lr 10.00 | ms/batch 4015.16 | loss  6.11 | ppl   450.62\n",
      "| epoch   9 |    91/   91 batches | lr 10.00 | ms/batch 3303.94 | loss  6.18 | ppl   482.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 515.59s | valid loss  0.20 | valid ppl     1.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |     1/   91 batches | lr 10.00 | ms/batch 7516.77 | loss 12.33 | ppl 225356.95\n",
      "| epoch  10 |     2/   91 batches | lr 10.00 | ms/batch 3894.02 | loss  6.06 | ppl   428.27\n",
      "| epoch  10 |     3/   91 batches | lr 10.00 | ms/batch 3887.83 | loss  6.02 | ppl   409.88\n",
      "| epoch  10 |     4/   91 batches | lr 10.00 | ms/batch 3832.78 | loss  6.10 | ppl   444.34\n",
      "| epoch  10 |     5/   91 batches | lr 10.00 | ms/batch 3786.81 | loss  6.11 | ppl   449.87\n",
      "| epoch  10 |     6/   91 batches | lr 10.00 | ms/batch 4006.62 | loss  6.18 | ppl   483.85\n",
      "| epoch  10 |     7/   91 batches | lr 10.00 | ms/batch 3809.42 | loss  6.15 | ppl   469.69\n",
      "| epoch  10 |     8/   91 batches | lr 10.00 | ms/batch 3837.27 | loss  6.10 | ppl   444.52\n",
      "| epoch  10 |     9/   91 batches | lr 10.00 | ms/batch 3917.71 | loss  6.09 | ppl   439.53\n",
      "| epoch  10 |    10/   91 batches | lr 10.00 | ms/batch 3875.21 | loss  6.05 | ppl   425.15\n",
      "| epoch  10 |    11/   91 batches | lr 10.00 | ms/batch 3913.04 | loss  6.14 | ppl   466.35\n",
      "| epoch  10 |    12/   91 batches | lr 10.00 | ms/batch 3829.48 | loss  6.15 | ppl   467.06\n",
      "| epoch  10 |    13/   91 batches | lr 10.00 | ms/batch 4010.10 | loss  6.09 | ppl   439.52\n",
      "| epoch  10 |    14/   91 batches | lr 10.00 | ms/batch 3697.59 | loss  6.18 | ppl   481.53\n",
      "| epoch  10 |    15/   91 batches | lr 10.00 | ms/batch 3932.87 | loss  6.11 | ppl   451.47\n",
      "| epoch  10 |    16/   91 batches | lr 10.00 | ms/batch 3784.88 | loss  6.12 | ppl   453.47\n",
      "| epoch  10 |    17/   91 batches | lr 10.00 | ms/batch 3953.78 | loss  6.13 | ppl   458.07\n",
      "| epoch  10 |    18/   91 batches | lr 10.00 | ms/batch 3855.80 | loss  6.13 | ppl   461.42\n",
      "| epoch  10 |    19/   91 batches | lr 10.00 | ms/batch 3847.19 | loss  6.12 | ppl   452.87\n",
      "| epoch  10 |    20/   91 batches | lr 10.00 | ms/batch 4004.85 | loss  6.14 | ppl   463.79\n",
      "| epoch  10 |    21/   91 batches | lr 10.00 | ms/batch 3790.40 | loss  6.03 | ppl   416.75\n",
      "| epoch  10 |    22/   91 batches | lr 10.00 | ms/batch 4224.51 | loss  6.04 | ppl   421.32\n",
      "| epoch  10 |    23/   91 batches | lr 10.00 | ms/batch 4036.22 | loss  6.21 | ppl   497.05\n",
      "| epoch  10 |    24/   91 batches | lr 10.00 | ms/batch 4373.87 | loss  6.08 | ppl   435.71\n",
      "| epoch  10 |    25/   91 batches | lr 10.00 | ms/batch 3866.47 | loss  6.08 | ppl   438.21\n",
      "| epoch  10 |    26/   91 batches | lr 10.00 | ms/batch 3856.09 | loss  6.00 | ppl   403.36\n",
      "| epoch  10 |    27/   91 batches | lr 10.00 | ms/batch 3718.60 | loss  6.21 | ppl   498.14\n",
      "| epoch  10 |    28/   91 batches | lr 10.00 | ms/batch 3800.12 | loss  6.18 | ppl   483.27\n",
      "| epoch  10 |    29/   91 batches | lr 10.00 | ms/batch 3819.26 | loss  6.13 | ppl   461.50\n",
      "| epoch  10 |    30/   91 batches | lr 10.00 | ms/batch 3819.62 | loss  6.17 | ppl   479.55\n",
      "| epoch  10 |    31/   91 batches | lr 10.00 | ms/batch 3873.26 | loss  6.21 | ppl   496.22\n",
      "| epoch  10 |    32/   91 batches | lr 10.00 | ms/batch 3757.68 | loss  6.08 | ppl   438.66\n",
      "| epoch  10 |    33/   91 batches | lr 10.00 | ms/batch 3755.52 | loss  6.08 | ppl   438.07\n",
      "| epoch  10 |    34/   91 batches | lr 10.00 | ms/batch 3974.22 | loss  6.06 | ppl   426.73\n",
      "| epoch  10 |    35/   91 batches | lr 10.00 | ms/batch 3785.02 | loss  6.01 | ppl   408.83\n",
      "| epoch  10 |    36/   91 batches | lr 10.00 | ms/batch 3883.39 | loss  6.10 | ppl   444.83\n",
      "| epoch  10 |    37/   91 batches | lr 10.00 | ms/batch 3863.31 | loss  6.07 | ppl   433.96\n",
      "| epoch  10 |    38/   91 batches | lr 10.00 | ms/batch 3899.43 | loss  6.11 | ppl   451.91\n",
      "| epoch  10 |    39/   91 batches | lr 10.00 | ms/batch 3983.02 | loss  6.13 | ppl   458.17\n",
      "| epoch  10 |    40/   91 batches | lr 10.00 | ms/batch 3876.67 | loss  6.09 | ppl   443.04\n",
      "| epoch  10 |    41/   91 batches | lr 10.00 | ms/batch 3780.80 | loss  6.11 | ppl   451.43\n",
      "| epoch  10 |    42/   91 batches | lr 10.00 | ms/batch 3878.25 | loss  6.11 | ppl   449.60\n",
      "| epoch  10 |    43/   91 batches | lr 10.00 | ms/batch 3810.62 | loss  6.03 | ppl   414.23\n",
      "| epoch  10 |    44/   91 batches | lr 10.00 | ms/batch 3837.50 | loss  6.09 | ppl   439.38\n",
      "| epoch  10 |    45/   91 batches | lr 10.00 | ms/batch 3835.07 | loss  6.14 | ppl   462.02\n",
      "| epoch  10 |    46/   91 batches | lr 10.00 | ms/batch 3762.08 | loss  6.04 | ppl   419.46\n",
      "| epoch  10 |    47/   91 batches | lr 10.00 | ms/batch 3829.81 | loss  6.14 | ppl   462.83\n",
      "| epoch  10 |    48/   91 batches | lr 10.00 | ms/batch 3774.48 | loss  6.04 | ppl   420.82\n",
      "| epoch  10 |    49/   91 batches | lr 10.00 | ms/batch 3883.41 | loss  6.15 | ppl   468.30\n",
      "| epoch  10 |    50/   91 batches | lr 10.00 | ms/batch 3839.84 | loss  6.08 | ppl   438.48\n",
      "| epoch  10 |    51/   91 batches | lr 10.00 | ms/batch 3890.40 | loss  6.14 | ppl   463.46\n",
      "| epoch  10 |    52/   91 batches | lr 10.00 | ms/batch 3812.24 | loss  6.00 | ppl   402.92\n",
      "| epoch  10 |    53/   91 batches | lr 10.00 | ms/batch 3950.60 | loss  6.01 | ppl   408.67\n",
      "| epoch  10 |    54/   91 batches | lr 10.00 | ms/batch 5034.08 | loss  6.10 | ppl   446.69\n",
      "| epoch  10 |    55/   91 batches | lr 10.00 | ms/batch 4231.83 | loss  6.12 | ppl   456.72\n",
      "| epoch  10 |    56/   91 batches | lr 10.00 | ms/batch 3880.01 | loss  6.23 | ppl   509.90\n",
      "| epoch  10 |    57/   91 batches | lr 10.00 | ms/batch 3858.25 | loss  6.11 | ppl   449.27\n",
      "| epoch  10 |    58/   91 batches | lr 10.00 | ms/batch 3919.44 | loss  6.12 | ppl   456.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |    59/   91 batches | lr 10.00 | ms/batch 3877.30 | loss  6.01 | ppl   407.19\n",
      "| epoch  10 |    60/   91 batches | lr 10.00 | ms/batch 3801.28 | loss  6.05 | ppl   425.89\n",
      "| epoch  10 |    61/   91 batches | lr 10.00 | ms/batch 3849.11 | loss  6.19 | ppl   489.57\n",
      "| epoch  10 |    62/   91 batches | lr 10.00 | ms/batch 4040.53 | loss  6.02 | ppl   410.52\n",
      "| epoch  10 |    63/   91 batches | lr 10.00 | ms/batch 4012.70 | loss  5.94 | ppl   381.22\n",
      "| epoch  10 |    64/   91 batches | lr 10.00 | ms/batch 3965.90 | loss  6.06 | ppl   429.86\n",
      "| epoch  10 |    65/   91 batches | lr 10.00 | ms/batch 3935.62 | loss  6.13 | ppl   458.28\n",
      "| epoch  10 |    66/   91 batches | lr 10.00 | ms/batch 3878.24 | loss  5.97 | ppl   393.33\n",
      "| epoch  10 |    67/   91 batches | lr 10.00 | ms/batch 3826.73 | loss  5.95 | ppl   384.61\n",
      "| epoch  10 |    68/   91 batches | lr 10.00 | ms/batch 3758.09 | loss  5.98 | ppl   393.78\n",
      "| epoch  10 |    69/   91 batches | lr 10.00 | ms/batch 3880.40 | loss  6.00 | ppl   401.94\n",
      "| epoch  10 |    70/   91 batches | lr 10.00 | ms/batch 3772.26 | loss  5.99 | ppl   397.66\n",
      "| epoch  10 |    71/   91 batches | lr 10.00 | ms/batch 3922.21 | loss  5.95 | ppl   383.30\n",
      "| epoch  10 |    72/   91 batches | lr 10.00 | ms/batch 3962.26 | loss  6.12 | ppl   456.85\n",
      "| epoch  10 |    73/   91 batches | lr 10.00 | ms/batch 3720.86 | loss  6.19 | ppl   489.31\n",
      "| epoch  10 |    74/   91 batches | lr 10.00 | ms/batch 3847.61 | loss  6.30 | ppl   544.49\n",
      "| epoch  10 |    75/   91 batches | lr 10.00 | ms/batch 3813.78 | loss  6.21 | ppl   495.43\n",
      "| epoch  10 |    76/   91 batches | lr 10.00 | ms/batch 3761.47 | loss  5.97 | ppl   392.08\n",
      "| epoch  10 |    77/   91 batches | lr 10.00 | ms/batch 3853.37 | loss  5.94 | ppl   381.44\n",
      "| epoch  10 |    78/   91 batches | lr 10.00 | ms/batch 3979.67 | loss  6.10 | ppl   445.64\n",
      "| epoch  10 |    79/   91 batches | lr 10.00 | ms/batch 3974.75 | loss  6.12 | ppl   454.77\n",
      "| epoch  10 |    80/   91 batches | lr 10.00 | ms/batch 3996.36 | loss  6.12 | ppl   457.00\n",
      "| epoch  10 |    81/   91 batches | lr 10.00 | ms/batch 3963.45 | loss  6.18 | ppl   483.46\n",
      "| epoch  10 |    82/   91 batches | lr 10.00 | ms/batch 3855.89 | loss  6.13 | ppl   460.31\n",
      "| epoch  10 |    83/   91 batches | lr 10.00 | ms/batch 3940.28 | loss  6.13 | ppl   460.66\n",
      "| epoch  10 |    84/   91 batches | lr 10.00 | ms/batch 3879.57 | loss  6.13 | ppl   459.72\n",
      "| epoch  10 |    85/   91 batches | lr 10.00 | ms/batch 3718.81 | loss  6.02 | ppl   410.93\n",
      "| epoch  10 |    86/   91 batches | lr 10.00 | ms/batch 4020.25 | loss  5.99 | ppl   397.92\n",
      "| epoch  10 |    87/   91 batches | lr 10.00 | ms/batch 3955.52 | loss  6.08 | ppl   438.67\n",
      "| epoch  10 |    88/   91 batches | lr 10.00 | ms/batch 3827.75 | loss  6.06 | ppl   429.14\n",
      "| epoch  10 |    89/   91 batches | lr 10.00 | ms/batch 3863.45 | loss  6.16 | ppl   473.65\n",
      "| epoch  10 |    90/   91 batches | lr 10.00 | ms/batch 3798.09 | loss  6.00 | ppl   404.30\n",
      "| epoch  10 |    91/   91 batches | lr 10.00 | ms/batch 2793.59 | loss  6.05 | ppl   424.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 382.75s | valid loss  0.20 | valid ppl     1.22\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LSTMModel' object has no attribute 'rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-be1f91d57274>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# After loading, the parameters are not a continuous chunk of memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# This makes them a continuous chunk, and will speed up the forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Run on test data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 576\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LSTMModel' object has no attribute 'rnn'"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  0.20 | test ppl     1.23\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # After loading, the parameters are not a continuous chunk of memory\n",
    "    # This makes them a continuous chunk, and will speed up the forward pass\n",
    "    model.lstm.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with your LSTM Language model, implement a function to generate a fixed number of words given an input text. You will need to use every part of the pipeline to: \n",
    "- Process the input text into words and then a series of word indexes using the vocabulary\n",
    "- Do inference on this input using the model\n",
    "- Loop in order to generate the next word **greedily** as many times as needed\n",
    "You can choose the next word by doing multinomial sampling from the output distribution (which you can control using softmax temperature) or simply using the argmax. Good Luck ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_sentence, n_generate = 5, max_history_length = seq_len, method = 'sampling', topk = 10):\n",
    "    \"\"\"\n",
    "     - input_sequence(string) : input\n",
    "     - n_generate (int) : number of words to generate\n",
    "     - max_history_length (int) : max length of the history to generate next words\n",
    "     - method : 'sampling' or 'greedy'\n",
    "     - topk (int) : sampling from k best words\n",
    "    \"\"\"\n",
    "    token_sequence = []\n",
    "    for w in input_sentence.split():\n",
    "        if w in corpus.dictionary.word2idx:\n",
    "            token_sequence.append(corpus.dictionary.word2idx[w])\n",
    "        else:\n",
    "            token_sequence.append(corpus.dictionary.word2idx['<unk>'])\n",
    "    # print(token_sequence)\n",
    "    \n",
    "    for _ in range(n_generate):\n",
    "        if len(token_sequence) > max_history_length:\n",
    "            token_input = token_sequence[-max_history_length:]\n",
    "        else:\n",
    "            token_input = token_sequence\n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=1)\n",
    "        inputs = torch.LongTensor(token_input).view(-1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(inputs, hidden, return_h = False)\n",
    "            \n",
    "            \n",
    "        if method == 'greedy':\n",
    "            predicted = torch.argmax(outputs[-1, 0])\n",
    "        elif method == 'sampling':\n",
    "            if topk < 0:\n",
    "                predicted = torch.multinomial(outputs[-1, 0].softmax(0), 1)\n",
    "            else:\n",
    "                values, indices = torch.topk(outputs[-1,0], topk)\n",
    "                predicted = indices[torch.multinomial(values, 1)]\n",
    "\n",
    "        token_sequence.append(predicted)\n",
    "    \n",
    "    return \" \".join([corpus.dictionary.idx2word[i] for i in token_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The film are one by \" and a most was that were the only with his team by which had well \\'s other @-@ first album \\'s album in The new game on its time on 1 to a other century and it has as not in her time with that was'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"The film\", n_generate = 50, max_history_length = 10, method = 'sampling', topk = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
